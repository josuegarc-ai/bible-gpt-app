# ================================================================
# âœ… Bible GPT â€” v2.9 (Personalized Plan Generator - Full & Integrated)
# ================================================================

# ==== Core imports ====
import os
import re
import json
import random
import urllib.parse
import tempfile
import subprocess
import requests
import shutil
from datetime import datetime
import streamlit as st

# ==== AI / NLP ====
import openai
import whisper
from thefuzz import fuzz # <-- REQUIRED FOR FUZZY STRING MATCHING

# ==== Web scraping ====
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

# ==== Media / YouTube ====
import yt_dlp
import imageio_ffmpeg

# ================================================================
# FFmpeg: supply binary via imageio-ffmpeg (no ffprobe required)
# ================================================================
_FFMPEG_BIN = imageio_ffmpeg.get_ffmpeg_exe()
_FFMPEG_DIR = os.path.dirname(_FFMPEG_BIN)

# Ensure PATH includes ffmpeg dir for subprocess/whisper/yt_dlp
os.environ["PATH"] = _FFMPEG_DIR + os.pathsep + os.environ.get("PATH", "")
# yt_dlp prefers a directory path here:
os.environ["FFMPEG_LOCATION"] = _FFMPEG_DIR

# ================================================================
# CONFIG
# ================================================================
BIBLE_API_BASE = "https://bible-api.com/"
VALID_TRANSLATIONS = ["web", "kjv", "asv", "bbe", "oeb-us"]

# OpenAI
client = openai.Client(api_key=st.secrets["OPENAI_API_KEY"])
MODEL = "gpt-4o"

# Diagnostic Quiz Questions (with "I don't know")
DIAGNOSTIC_QUESTIONS = [
    {
        "question": "Who led the Israelites out of Egypt?",
        "options": ["Abraham", "Moses", "David", "Noah", "I don't know"],
        "correct": "Moses"
    },
    {
        "question": "Which disciple denied Jesus three times before the rooster crowed?",
        "options": ["Judas", "John", "Peter", "Thomas", "I don't know"],
        "correct": "Peter"
    },
    {
        "question": "What theological term refers to the study of 'last things' or end times?",
        "options": ["Soteriology", "Eschatology", "Christology", "Pneumatology", "I don't know"],
        "correct": "Eschatology"
    }
]

# ================================================================
# CHAT CONFIG (FINAL GENERALIZED CONVICTION PROMPT)
# ================================================================
CHAT_HISTORY_FILE = "user_chat_history.json"

GENERAL_SYSTEM_PROMPT = """
You are Bible GPT, a mentor who speaks with the love and authority of **absolute biblical truth**.
Your primary goal is to guide the user toward righteousness and holiness by clearly stating what the Bible teaches.

- **Tone:** Be conversational, loving, and pastoral, but also **firm, direct, and unwavering in the truth**.Â 
- **Format:** Your answers must be **concise and conversational**. Do NOT use long, numbered lists. Give your answer in a direct paragraph.

- **Role:** You are not a neutral counselor. You are a Bible-based guide providing the truth as it is written.

- **Core Instruction - The Two Paths:**
You MUST first determine if the user's question is about a **Clear Biblical Command** or a **Disputable Matter**.

1.Â  **If it is a Clear Biblical Command:**
Â  Â  - This includes any topic the Bible clearly *condemns* (e.g., idolatry, sexual immorality, occult/pagan practices) or *commands*.
Â  Â  - You must state the biblical position as a **clear "yes" or "no" truth**. (e.g., "No, as followers of Christ, we are called to avoid that.")
Â  Â  - Immediately back up your answer with 1-2 relevant scriptures.
Â  Â  - **CRITICAL: You MUST NOT mention "personal conviction," "disputable matters," or "Romans 14" in this type of answer. These topics are not disputable, and mentioning them is confusing and weak.**

2.Â  **If it is a Disputable Matter (Romans 14):**
Â  Â  - This *only* applies to topics where the Bible *does not* give a direct command (e.g., eating certain foods, observing certain Sabbath days).
Â  Â  - **Only** for these topics may you explain that it is a matter of personal conviction before God, citing Romans 14.

- **Default:** When in doubt, default to the **Clear Biblical Command** path.
"""

THEOLOGICAL_SYSTEM_PROMPT = """
You are the "Theological Scholar" mode of Bible GPT.
Your goal is not just academic knowledge, but **deep spiritual conviction** built on doctrinal truth.
- **Tone & Format:** You are an expert theologian who speaks with authority. Be **concise, direct, and authoritative**. Your response should be a sharp, insightful paragraph, not a long academic list.
- **Depth:** Provide deep, analytical, and comprehensive theological answers. Cite theological concepts, historical context, and original languages where appropriate.
- **Tools:** You have a web search tool. Use it *any time* the user asks for connections between prophecy, scripture, and **current events**.

- **Core Instruction - The Two Paths (Scholarly Application):**
You MUST apply this core logic.

1.Â  **If it is a Clear Biblical Command:**
Â  Â  - This includes any topic the Bible clearly *condemns* (e.g., idolatry, occult practices).
Â  Â  - You must state the biblical position as **doctrinal truth**. Explain *why* it is theologically non-negotiable.
Â  Â  - (Example: For Halloween, identify its pagan origins (Samhain) as a form of occultism, which the Bible clearly condemns. This makes it a matter of **truth** (2 Cor 6:14), not "adiaphora.")
Â  Â  - **CRITICAL: You MUST NOT mention "disputable matters" (adiaphora) or "Romans 14" when discussing a topic of clear biblical condemnation. These concepts are mutually exclusive.**

2.Â  **If it is a Disputable Matter (Adiaphora):**
Â  Â  - This *only* applies to topics where the Bible *does not* give a direct command.
Â  Â  - You must identify these topics as "adiaphora" (things indifferent) and explain the principle of Christian liberty based on Romans 14.
"""

# ================================================================
# UTILITIES
# ================================================================
def fetch_bible_verse(passage: str, translation: str = "web") -> str:
    """Fetches a Bible passage from bible-api.com."""
    if translation not in VALID_TRANSLATIONS:
        raise ValueError(f"Unsupported translation. Choose from: {VALID_TRANSLATIONS}")
    encoded_passage = urllib.parse.quote(passage.strip())
    url = f"{BIBLE_API_BASE}{encoded_passage}?translation={translation}"
    try:
        resp = requests.get(url, timeout=12)
        if resp.status_code != 200:
            raise Exception(f"Error {resp.status_code}: Unable to fetch passage. Check reference format.")
        data = resp.json()
        text = data.get("text", "").strip()
        if not text:
            raise Exception("Passage returned no text. Check book/chapter/verse.")
        return text
    except requests.RequestException as e:
        raise Exception(f"Network error: {e}")
    except Exception as e:
        raise Exception(f"API Error: {e}")

def ask_gpt_conversation(prompt: str) -> str:
Â  Â  """Stable, conservative GPT call for summaries and guidance."""
Â  Â  r = client.chat.completions.create(
Â  Â  Â  Â  model=MODEL,
Â  Â  Â  Â  temperature=0.3,
Â  Â  Â  Â  max_tokens=1000,
Â  Â  Â  Â  messages=[
Â  Â  Â  Â  Â  Â  {"role": "system", "content": "You are a biblical mentor and teacher. You explain Scripture clearly, compassionately, and apply it to modern life with spiritual insight."},
Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt},
Â  Â  Â  Â  ],
Â  Â  )
Â  Â  return r.choices[0].message.content.strip()

def extract_json_from_response(response_text: str):
Â  Â  """Legacy JSON extractor for simple objects."""
Â  Â  try:
Â  Â  Â  Â  # Prioritize JSON within markdown code blocks
Â  Â  Â  Â  match = re.search(r"```json\s*(\{.*?\})\s*```", response_text, re.DOTALL)
Â  Â  Â  Â  if match:
Â  Â  Â  Â  Â  Â  json_text = match.group(1)
Â  Â  Â  Â  else:
Â Â  Â  Â  Â  Â  Â  # Fallback to finding the first curly brace object
Â  Â  Â  Â  Â  Â  json_text_match = re.search(r"\{.*\}", response_text, re.DOTALL)
Â  Â  Â  Â  Â  Â  if not json_text_match: return None
Â  Â  Â  Â  Â  Â  json_text = json_text_match.group(0)
Â  Â  Â  Â  return json.loads(json_text)
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Error extracting JSON: {e}") # Added error logging
Â  Â  Â  Â  return None

# ================================================================
# CHAT UTILITIES
# ================================================================

def load_chat_history() -> list:
Â  Â  """Loads chat history from a local JSON file."""
Â  Â  if os.path.exists(CHAT_HISTORY_FILE):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  with open(CHAT_HISTORY_FILE, "r", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  Â  Â  return json.load(f)
Â  Â  Â  Â  except json.JSONDecodeError:
Â  Â  Â  Â  Â  Â  return [] # Return empty if file is corrupt
Â  Â  return []

def save_chat_history(history: list):
Â  Â  """Saves chat history to a local JSON file."""
Â  Â  try:
Â  Â  Â  Â  with open(CHAT_HISTORY_FILE, "w", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  json.dump(history, f, indent=2)
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Failed to save chat history: {e}")
Â Â  Â  Â  Â 
def get_chat_messages(history: list, max_turns: int = 20) -> list:
Â  Â  """Manages chat history to prevent token overflow by summarizing old messages."""
Â  Â  if len(history) <= max_turns:
Â  Â  Â  Â  return history # Return full history if it's short

Â  Â  # History is too long, summarize the oldest part
Â  Â  # Keep the most recent 10 messages (5 turns)
Â  Â  messages_to_keep = history[-10:]
Â  Â  messages_to_summarize = history[:-10]
Â Â  Â 
Â  Â  # Create a text blob of the old chat
Â  Â  old_chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in messages_to_summarize])
Â Â  Â 
Â  Â  # Use your existing function to summarize
Â  Â  try:
Â  Â  Â  Â  summary_prompt = f"Concisely summarize the key points of this conversation in one paragraph: {old_chat_text}"
Â  Â  Â  Â  summary = ask_gpt_conversation(summary_prompt) # This is your existing function
Â  Â  except Exception as e:
Â  Â  Â  Â  st.warning(f"Could not summarize history: {e}")
Â  Â  Â  Â  summary = "Summary of prior conversation is unavailable."

Â  Â  # Return a new history object
Â  Â  return [
Â  Â  Â  Â  {"role": "system", "content": f"[Prior Conversation Summary]: {summary}"},
Â  Â  Â  Â  *messages_to_keep
Â  Â  ]

def web_search(query: str) -> str:
Â  Â  """Performs a web search using DuckDuckGo."""
Â  Â  st.caption(f"ğŸ” Searching the web for: '{query}'") # Show the user it's searching
Â  Â  try:
Â  Â  Â  Â  with DDGS() as ddgs:
Â  Â  Â  Â  Â  Â  # We will format the results as a clean string for the AI
Â  Â  Â  Â  Â  Â  results = [r for r in ddgs.text(query, max_results=5)]
Â  Â  Â  Â  Â  Â  if not results:
Â  Â  Â  Â  Â  Â  Â  Â  return "No relevant web results found."
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # Format this for the AI to read
Â  Â  Â  Â  Â  Â  formatted_results = "\n".join([
Â  Â  Â  Â  Â  Â  Â  Â  f"- Snippet: {r['body']}\nÂ  Source: {r['href']}"Â 
Â  Â  Â  Â  Â  Â  Â  Â  for r in results
Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  return f"Here are the web search results:\n{formatted_results}"
Â Â  Â  Â  Â  Â  Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"Search failed. Error: {str(e)}"
Â Â  Â  Â  Â 
# ================================================================
# SERMON SEARCH (YouTube result links via HTML scrape)
# ================================================================
def search_sermons_online(passage: str):
Â  Â  headers = {"User-Agent": "Mozilla/5.0"}
Â  Â  pastors = ["Philip Anthony Mitchell", "TD Jakes", "Tony Evans", "Mike Todd"]
Â  Â  base_url = "https://www.youtube.com/results?search_query="
Â  Â  results = []

Â  Â  for pastor in pastors:
Â  Â  Â  Â  query = f"{pastor} sermon on {passage}"
Â  Â  Â  Â  search_url = base_url + urllib.parse.quote(query)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  response = requests.get(search_url, headers=headers, timeout=12)
Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(response.text, "html.parser")
Â  Â  Â  Â  Â  Â  scripts = soup.find_all("script")
Â  Â  Â  Â  Â  Â  found = False

Â  Â  Â  Â  Â  Â  for script in scripts:
Â  Â  Â  Â  Â  Â  Â  Â  txt = script.text or ""
Â  Â  Â  Â  Â  Â  Â  Â  if "var ytInitialData" in txt:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  start = txt.find("var ytInitialData") + len("var ytInitialData = ")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  end = txt.find("};", start) + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if start > -1 and end > start:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json_text = txt[start:end]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  yt_data = json.loads(json_text)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Added checks for key existence
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contents = yt_data.get("contents",{}).get("twoColumnSearchResultsRenderer",{}).get("primaryContents",{}).get("sectionListRenderer",{}).get("contents",[{}])[0].get("itemSectionRenderer",{}).get("contents",[])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for item in contents:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "videoRenderer" in item:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_id = item["videoRenderer"]["videoId"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_url = f"https://www.youtube.com/watch?v={video_id}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append({"pastor": pastor, "url": video_url})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  found = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break # Found first result for this pastor
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if found: break # Move to next pastor
Â  Â  Â  Â  Â  Â  if not found:
Â  Â  Â  Â  Â  Â  Â  Â  results.append({"pastor": pastor, "url": "âŒ No result"})
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  results.append({"pastor": pastor, "url": f"âŒ Error: {e}"})
Â  Â  return results

# ================================================================
# BIBLE LOOKUP MODE
# ================================================================
def run_bible_lookup():
Â  Â  st.subheader("ğŸ“– Bible Lookup")
Â  Â  passage = st.text_input("Enter a Bible passage (e.g., John 3:16):")
Â  Â  translation = st.selectbox("Choose translation:", VALID_TRANSLATIONS)
Â  Â  if st.button("Fetch Verse") and passage:
Â  Â  Â  Â  with st.spinner("Fetching and analyzing..."):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  verse_text = fetch_bible_verse(passage, translation)
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"**{passage.strip()} ({translation.upper()})**\n\n{verse_text}")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  Â  Â  Â  Â  summary = ask_gpt_conversation(f"Summarize and explain this Bible verse clearly: '{verse_text}' ({passage}). Include a daily life takeaway.")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ’¡ AI Summary & Takeaway:**")
Â  Â  Â  Â  Â  Â  Â  Â  st.info(summary)

Â  Â  Â  Â  Â  Â  Â  Â  cross = ask_gpt_conversation(f"List 2â€“3 cross-referenced Bible verses related to: '{verse_text}' and briefly explain their connection.")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ”— Cross References:**")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(cross)

Â  Â  Â  Â  Â  Â  Â  Â  sermons = search_sermons_online(passage)
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ™ï¸ Related Sermons:**")
Â  Â  Â  Â  Â  Â  Â  Â  for item in sermons:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"- {item['pastor']}: {item['url']}")
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(str(e))

# ================================================================
# CHAT MODE (FINAL VERSION - STATEFUL, CONVICTING, & WEB-ENABLED)
# ================================================================
def run_chat_mode():
Â  Â  st.subheader("ğŸ’¬ Chat with GPT")
Â Â  Â 
Â  Â  is_theological_mode = st.toggle(
Â  Â  Â  Â  "Enable Deep Theological Chat",Â 
Â  Â  Â  Â  value=False,
Â  Â  Â  Â  help="Toggle on for in-depth, scholarly answers with web search for current events."
Â  Â  )

Â  Â  if "chat_history" not in st.session_state:
Â  Â  Â  Â  st.session_state.chat_history = load_chat_history()

Â  Â  st.markdown("---")
Â  Â  chat_container = st.container(height=400, border=False)
Â  Â  with chat_container:
Â  Â  Â  Â  if not st.session_state.chat_history:
Â Â  Â  Â  Â  Â  Â  st.caption("Your conversation will appear here. Your chat history is saved automatically.")
Â Â  Â  Â  Â 
Â  Â  Â  Â  for msg in st.session_state.chat_history:
Â  Â  Â  Â  Â  Â  # Don't show the tool call/result messages to the user, only human/ai
Â  Â  Â  Â  Â  Â  if msg["role"] in ["user", "assistant"]:
Â  Â  Â  Â  Â  Â  Â  Â  who = "âœï¸ Bible GPT" if msg["role"] == "assistant" else "ğŸ§ You"
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{who}:** {msg['content']}")
Â  Â  st.markdown("---")
Â Â  Â 
Â  Â  user_input = st.text_input("Ask a question or share a thought:")
Â Â  Â 
Â  Â  if st.button("Send", type="primary") and user_input:
Â Â  Â  Â  Â 
Â  Â  Â  Â  if user_input.lower().strip() in ["exit", "quit", "end", "stop"]:
Â  Â  Â  Â  Â  Â  st.info("Conversation ended. Your history is saved.")
Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  st.session_state.chat_history.append({"role": "user", "content": user_input})
Â  Â  Â  Â  st.rerun() # Re-run to show the user's message immediately

Â  Â  # --- This block handles processing the chat after the user message is added ---
Â  Â  # Check if the last message was from the user, meaning AI needs to respond
Â  Â  if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
Â  Â  Â  Â  with st.spinner("Thinking..."):
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # 1. Select prompt and tools
Â  Â  Â  Â  Â  Â  system_prompt = THEOLOGICAL_SYSTEM_PROMPT if is_theological_mode else GENERAL_SYSTEM_PROMPT
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # Only give the web search tool to the "Theological" mode
Â  Â  Â  Â  Â  Â  tools = [
Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "type": "function",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "function": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "name": "web_search",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "description": "Searches the internet for current events, news, or topics. Use this to connect prophecy or biblical topics to the modern world.",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "parameters": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "type": "object",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "properties": {"query": {"type": "string", "description": "The search query"}},
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "required": ["query"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  ] if is_theological_mode else None

Â  Â  Â  Â  Â  Â  # 2. Get managed message history
Â  Â  Â  Â  Â  Â  messages_for_api = get_chat_messages(st.session_state.chat_history)
Â  Â  Â  Â  Â  Â  final_messages = [{"role": "system", "content": system_prompt}] + messages_for_api
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # 3. Call the AI
Â  Â  Â  Â  Â  Â  Â  Â  response = client.chat.completions.create(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=MODEL,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=final_messages,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.3, # Lower temp for more direct, factual answers
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tools=tools
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  response_message = response.choices[0].message

Â  Â  Â  Â  Â  Â  Â  Â  # 4. Check if AI wants to use a tool (web search)
Â  Â  Â  Â  Â  Â  Â  Â  if response_message.tool_calls:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append(response_message) # Save the AI's tool request
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- This is the new Tool-Calling Loop ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for tool_call in response_message.tool_calls:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  function_name = tool_call.function.name
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if function_name == "web_search":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Get the query from the AI
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  function_args = json.loads(tool_call.function.arguments)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  query = function_args.get("query")
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Call our Python web_search function
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  function_response = web_search(query=query)
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Send the search results back to the AI
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "tool_call_id": tool_call.id,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "role": "tool",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "name": function_name,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "content": function_response,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 5. Call AI *AGAIN* with the search results
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # This lets the AI form a final answer
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  second_response = client.chat.completions.create(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=MODEL,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=st.session_state.chat_history, # Send the *full* history including tool results
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_reply = second_response.choices[0].message.content.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append({"role": "assistant", "content": final_reply})

Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 6. No tool was needed, just a direct answer
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_reply = response_message.content.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append({"role": "assistant", "content": final_reply})

Â  Â  Â  Â  Â  Â  Â  Â  # 7. Save and refresh
Â  Â  Â  Â  Â  Â  Â  Â  save_chat_history(st.session_state.chat_history)
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error communicating with AI: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.pop() # Remove the user's message if it failed
Â  Â  Â  Â  Â  Â  Â  Â  save_chat_history(st.session_state.chat_history)
# ================================================================
# PIXAR STORY ANIMATION
# ================================================================
def run_pixar_story_animation():
Â  Â  st.subheader("ğŸ¥ Pixar-Style Animated Bible Story")
Â  Â  st.info("Generate a biblically accurate Pixar-style short film scene-by-scene based on Scripture.")
Â  Â  book = st.text_input("ğŸ“˜ Bible book (e.g., Exodus):")
Â  Â  chapter = st.text_input("ğŸ”¢ Chapter (optional):")
Â  Â  tone = st.selectbox("ğŸ­ Pixar tone:", ["Adventurous", "Heartwarming", "Funny", "Epic", "All Ages"])
Â  Â  theme = st.text_input("ğŸ’¡ Lesson or theme (e.g., faith, obedience):")
Â  Â  if st.button("ğŸ¬ Generate Pixar Story") and book:
Â  Â  Â  Â  reference = f"{book} {chapter}".strip() if chapter else book
Â  Â  Â  Â  story_prompt = f"Turn the Bible story from {reference} into a Pixar-studio style film story for kids ages 4â€“10. Tone: {tone}. Theme: {theme or 'faith'}. Break it into exactly 5 cinematic scenes with 1â€“2 sentences each. Each scene should be visually imaginative but true to the biblical setting. Output as a numbered list."
Â  Â  Â  Â  response = ask_gpt_conversation(story_prompt)
Â  Â  Â  Â  st.markdown("### ğŸ“š Pixar-Style Bible Story Scenes")
Â  Â  Â  Â  # Improved parsing for numbered lists or simple newlines
Â  Â  Â  Â  scenes = re.findall(r"^\d+\.\s*(.*)", response, re.MULTILINE) or [s.strip() for s in response.split("\n") if s.strip() and not s.strip().isdigit()]
Â  Â  Â  Â  if not scenes:
Â  Â  Â  Â  Â  Â  st.error("âŒ Could not parse story scenes. Try different input.")
Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  for idx, scene in enumerate(scenes[:5], 1): # Limit to 5 scenes
Â  Â  Â  Â  Â  Â  st.markdown(f"#### ğŸ¬ Scene {idx}\n*{scene}*")

# ================================================================
# PRACTICE CHAT (Quiz) - UPDATED FOR FUZZY MATCHING AND EXPLANATION
# ================================================================
def run_practice_chat():
Â  Â  st.subheader("ğŸ¤  Practice Chat")
Â  Â  if "practice_state" not in st.session_state:
Â  Â  Â  Â  st.session_state.practice_state = {
Â  Â  Â  Â  Â  Â  "questions": [], "current": 0, "score": 0, "book": "", "style": "", "level": "",
Â  Â  Â  Â  Â  Â  "awaiting_next": False, "used_questions": set(), "used_phrases": set(), "restart_flag": False,
Â  Â  Â  Â  }
Â  Â  S = st.session_state.practice_state

Â  Â  if S.get("restart_flag"):
Â  Â  Â  Â  st.session_state.practice_state = {
Â  Â  Â  Â  Â  Â  "questions": [], "current": 0, "score": 0, "book": "", "style": "", "level": "",
Â  Â  Â  Â  Â  Â  "awaiting_next": False, "used_questions": set(), "used_phrases": set(), "restart_flag": False,
Â  Â  Â  Â  }
Â  Â  Â  Â  st.rerun()

Â  Â  if not S["questions"]:
Â  Â  Â  Â  random_practice = st.checkbox("ğŸ“– Random questions from the Bible")
Â  Â  Â  Â  book = "" if random_practice else st.text_input("Enter Bible book:")
Â  Â  Â  Â  style = st.selectbox("Choose question style:", ["multiple choice", "fill in the blank", "true or false", "mixed"])
Â  Â  Â  Â  level = st.selectbox("Select your understanding level:", ["beginner", "intermediate", "advanced"])

Â  Â  Â  Â  if st.button("Start Practice") and (random_practice or book):
Â  Â  Â  Â  Â  Â  S["book"] = book; S["style"] = style; S["level"] = level
Â  Â  Â  Â  Â  Â  num_questions = random.randint(7, 10)
Â  Â  Â  Â  Â  Â  with st.spinner("Generating practice questions..."): # Added spinner
Â  Â  Â  Â  Â  Â  Â  Â  while len(S["questions"]) < num_questions:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chosen_style = style if style != "mixed" else random.choice(["multiple choice", "fill in the blank", "true or false"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic = book if book else "the Bible"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  q_prompt = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Generate a {chosen_style} Bible question from {topic} suitable for a {level} learner. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Format as JSON with 'question', 'correct', 'choices' (list of strings), and 'question_type' ('multiple_choice', 'fill_in_the_blank', or 'true_false'). " # Added question_type
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{'Choices for true/false should be [\"True\", \"False\"].' if chosen_style == 'true or false' else 'For multiple choice, include 1 correct and 3 incorrect options.'}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = extract_json_from_response(ask_gpt_conversation(q_prompt)) # Using more robust extractor
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not data or 'question' not in data or 'correct' not in data: continue # Basic validation
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add question_type if missing (shouldn't happen with updated prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'question_type' not in data: data['question_type'] = chosen_style

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  norm = data["question"].strip().lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if norm in S["used_questions"]: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["used_questions"].add(norm)
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Deduplicate options and ensure correct is present for MC/FITB
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if data['question_type'] != 'true_false':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'choices' not in data: data['choices'] = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Ensure choices is a list
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not isinstance(data['choices'], list): data['choices'] = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Deduplicate while preserving order (important for potential distractors)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  seen = set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uniq = [x for x in data['choices'] if not (x in seen or seen.add(x))]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add correct answer if not already in choices
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if data['correct'] not in seen:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uniq.append(data['correct'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  random.shuffle(uniq)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data['choices'] = uniq
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data['choices'] = ["True", "False"] # Standardize TF choices

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["questions"].append(data)
Â  Â  Â  Â  Â  Â  if not S["questions"]: # Handle case where generation failed
Â  Â  Â  Â  Â  Â  Â  Â  st.error("Failed to generate questions. Please try again.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  elif S["current"] < len(S["questions"]):
Â  Â  Â  Â  q = S["questions"][S["current"]]
Â  Â  Â  Â  st.markdown(f"**Q{S['current'] + 1}: {q['question']}**")
Â Â  Â  Â  Â 
Â  Â  Â  Â  q_type = q.get("question_type", "multiple_choice") # Default if missing
Â  Â  Â  Â  ans = None
Â  Â  Â  Â  if q_type == 'multiple_choice':
Â Â  Â  Â  Â  Â  Â  ans = st.radio("Choose:", q.get("choices", []), key=f"q{S['current']}_choice", index=None)
Â  Â  Â  Â  elif q_type == 'true_false':
Â Â  Â  Â  Â  Â  Â  ans = st.radio("Choose:", ["True", "False"], key=f"q{S['current']}_choice", index=None)
Â  Â  Â  Â  elif q_type == 'fill_in_the_blank':
Â Â  Â  Â  Â  Â  Â  ans = st.text_input("Fill in the blank:", key=f"q{S['current']}_choice")

Â  Â  Â  Â  if not S.get("awaiting_next", False):
Â  Â  Â  Â  Â  Â  if st.button("Submit Answer"):
Â  Â  Â  Â  Â  Â  Â  Â  if ans is None and (q_type == 'multiple_choice' or q_type == 'true_false'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please select an answer.")
Â  Â  Â  Â  Â  Â  Â  Â  elif ans is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if _answers_match(ans, q["correct"], q_type): # Using fuzzy match function
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["score"] += 1; st.success("âœ… Correct!"); S["current"] += 1; st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Incorrect. Correct answer: **{q['correct']}**"); # Made correct answer bold
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Updated explanation prompt to include the incorrect answer
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  explain_prompt = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"You're a theological Bible teacher. A student answered '{ans}' to the question: '{q['question']}'. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"The correct answer is '{q['correct']}'. Explain briefly why their answer '{ans}' was incorrect and why '{q['correct']}' is the right one, using Scripture-based reasoning if possible."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  explanation = ask_gpt_conversation(explain_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ“œ Teaching Moment:**"); st.write(explanation); S["awaiting_next"] = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to show teaching moment and Next button
Â Â  Â  Â  Â 
Â  Â  Â  Â  # Display Next button only after an incorrect answer's explanation is shown
Â  Â  Â  Â  if S.get("awaiting_next"):
Â  Â  Â  Â  Â  Â  if st.button("Next Question", key=f"next_{S['current']}"):
Â  Â  Â  Â  Â  Â  Â  Â  S["awaiting_next"] = False # Reset flag
Â  Â  Â  Â  Â  Â  Â  Â  S["current"] += 1
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  else:
Â  Â  Â  Â  st.markdown(f"**ğŸŒ Final Score: {S['score']}/{len(S['questions'])}**")
Â  Â  Â  Â  if st.button("Restart Practice"): S["restart_flag"] = True; st.rerun()

# ================================================================
# FAITH JOURNAL
# ================================================================
def run_faith_journal():
Â  Â  st.subheader("ğŸ“ Faith Journal")
Â  Â  entry = st.text_area("Write your thoughts, prayers, or reflections:")
Â  Â  if st.button("Save Entry") and entry:
Â  Â  Â  Â  ts = datetime.now().strftime("%Y%m%d_%H%M%S")
Â  Â  Â  Â  # Ensure directory exists
Â  Â  Â  Â  os.makedirs("faith_journal", exist_ok=True)
Â  Â  Â  Â  filename = os.path.join("faith_journal", f"journal_{ts}.txt")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  with open(filename, "w", encoding="utf-8") as f: f.write(entry)
Â  Â  Â  Â  Â  Â  st.success(f"Saved as `{filename}`.")
Â  Â  Â  Â  Â  Â  # Added option for AI insight after saving
Â  Â  Â  Â  Â  Â  if st.checkbox("Get spiritual insight from this entry?"):
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Analyzing your entry..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  insight = ask_gpt_conversation(f"Analyze this faith journal entry and offer spiritual insight and encouragement based on biblical principles: {entry}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ’¡ Insight:**"); st.write(insight)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Failed to save entry: {e}")

# ================================================================
# TAILORED LEARNING PATH (Kept for compatibility, but Learn Module is preferred)
# ================================================================
def run_learning_path_mode():
Â  Â  st.subheader("ğŸ“š Tailored Learning Path (Legacy)")
Â  Â  st.warning("Consider using the 'Learn Module' for a more interactive experience.")
Â  Â  user_type = st.selectbox("User type:", ["child", "adult"])
Â  Â  goal = st.text_input("Learning goal:")
Â  Â  level = st.selectbox("Bible knowledge level:", ["beginner", "intermediate", "advanced"])
Â  Â  styles = st.multiselect(
Â  Â  Â  Â  "Preferred learning styles:",
Â  Â  Â  Â  ["storytelling", "questions", "memory games", "reflection", "devotional"],
Â  Â  )
Â  Â  if st.button("Generate Legacy Path") and goal and styles:
Â  Â  Â  Â  style_str = ", ".join(styles)
Â  Â  Â  Â  prompt = (f"Design a creative Bible learning path outline for a {user_type} with goal '{goal}', level '{level}', "
Â  Â  Â  Â  Â  Â  Â  Â  Â  f"using these learning styles: {style_str}. Provide a list of suggested topics or activities.")
Â  Â  Â  Â  result = ask_gpt_conversation(prompt)
Â  Â  Â  Â  st.text_area("ğŸ“˜ Learning Path Outline", result, height=500)

# ================================================================
# BIBLE BETA
# ================================================================
def run_bible_beta():
Â  Â  st.subheader("ğŸ“˜ Bible Beta Mode")
Â  Â  st.info("ğŸ§ª Experimental: Read Bible chapters.")
Â  Â  book = st.text_input("Book (e.g., John):")
Â  Â  # Changed to text input for flexibility, e.g., "John 3"
Â  Â  passage_ref = st.text_input("Chapter or Passage (e.g., 3 or 3:1-16):", "1")Â 
Â  Â  translation_beta = st.selectbox("Translation:", VALID_TRANSLATIONS, key="beta_trans")

Â  Â  if st.button("Display Passage") and book and passage_ref:
Â  Â  Â  Â  full_ref = f"{book} {passage_ref}"
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  with st.spinner(f"Fetching {full_ref}..."):
Â  Â  Â  Â  Â  Â  Â  Â  text = fetch_bible_verse(full_ref, translation_beta)
Â  Â  Â  Â  Â  Â  Â  Â  st.text_area(f"ğŸ“– {full_ref} ({translation_beta.upper()})", value=text, height=400)
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # Optional summarization integrated below text area
Â  Â  Â  Â  Â  Â  if st.checkbox("âœ¨ Summarize this passage?"):
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Generating summary..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary = ask_gpt_conversation(f"Summarize and explain the key points of this Bible passage: {text} ({full_ref})")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ’¬ Summary & Key Points:**"); st.markdown(summary)

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Error fetching passage: {e}")

# ================================================================
# SERMON TRANSCRIBER & SUMMARIZER (YouTube or file upload)
# ================================================================
def _convert_to_wav_if_needed(src_path: str) -> str:
Â  Â  """If Whisper has trouble with container, convert to 16k mono WAV using ffmpeg."""
Â  Â  with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
Â  Â  Â  Â  wav_path = tmp_file.name
Â  Â  cmd = [_FFMPEG_BIN, "-y", "-i", src_path, "-ar", "16000", "-ac", "1", wav_path]
Â  Â  try:
Â  Â  Â  Â  # Added timeout and capture stderr
Â  Â  Â  Â  result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=120)Â 
Â  Â  except subprocess.CalledProcessError as e:
Â  Â  Â  Â  raise Exception(f"ffmpeg conversion failed with exit code {e.returncode}: {e.stderr}")
Â  Â  except subprocess.TimeoutExpired:
Â  Â  Â  Â  raise Exception("ffmpeg conversion timed out after 2 minutes.")
Â  Â  return wav_path

def download_youtube_audio(url: str) -> tuple[str, str, str]:
Â  Â  """Download audio *without* postprocessing (so yt_dlp won't call ffprobe)."""
Â  Â  # Use a specific file extension preferred by yt-dlp format selection
Â  Â  with tempfile.NamedTemporaryFile(delete=False, suffix=".m4a") as temp_file:Â 
Â  Â  Â  Â  output_path = temp_file.name
Â Â  Â 
Â  Â  ydl_opts = {
Â  Â  Â  Â  # Prefer m4a, fallback to best audio
Â  Â  Â  Â  "format": "bestaudio[ext=m4a]/bestaudio/best",Â 
Â  Â  Â  Â  "outtmpl": output_path, # Use the temp file path directly
Â  Â  Â  Â  "ffmpeg_location": os.environ.get("FFMPEG_LOCATION", _FFMPEG_DIR),
Â  Â  Â  Â  "quiet": True,Â 
Â  Â  Â  Â  "retries": 3,Â 
Â  Â  Â  Â  "noprogress": True,
Â  Â  Â  Â  "http_headers": { # Standard browser headers
Â  Â  Â  Â  Â  Â  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
Â  Â  Â  Â  Â  Â  "Accept-Language": "en-US,en;q=0.9",Â 
Â  Â  Â  Â  Â  Â  "Referer": "https://www.youtube.com/",
Â  Â  Â  Â  },
Â  Â  Â  Â  # Use cookies if available
Â  Â  Â  Â  **({"cookiefile": "cookies.txt"} if os.path.exists("cookies.txt") else {}),Â 
Â  Â  Â  Â  # Set socket timeout
Â  Â  Â  Â  "socket_timeout": 30,
Â  Â  }
Â  Â  try:
Â  Â  Â  Â  # Use context manager for yt_dlp
Â  Â  Â  Â  with yt_dlp.YoutubeDL(ydl_opts) as ydl:Â 
Â  Â  Â  Â  Â  Â  info = ydl.extract_info(url, download=True)
Â  Â  Â  Â  Â  Â  title = info.get("title", "Untitled Sermon")
Â  Â  Â  Â  Â  Â  uploader = info.get("uploader", "Unknown")
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  # Check if file exists and is not empty AFTER download attempt
Â  Â  Â  Â  if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:
Â  Â  Â  Â  Â  Â  raise Exception("Audio download failed or resulted in an empty file.")
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  return output_path, uploader, title
Â  Â  except yt_dlp.utils.DownloadError as e: # Catch specific download errors
Â  Â  Â  Â  raise Exception(f"âŒ YouTube download error: {e}")
Â  Â  except Exception as e:
Â  Â  Â  Â  raise Exception(f"âŒ Failed during YouTube audio processing: {e}")

def run_sermon_transcriber():
Â  Â  st.subheader("ğŸ§ Sermon Transcriber & Summarizer")
Â  Â  st.info("Upload sermon audio or paste a YouTube link (max ~15-20 mins recommended due to processing limits).")
Â  Â  yt_link = st.text_input("ğŸ“º YouTube Link:")
Â  Â  audio_file = st.file_uploader("ğŸ™ï¸ Or upload audio (MP3/WAV/M4A/MP4):", type=["mp3", "wav", "m4a", "mp4"])

Â  Â  if st.button("âºï¸ Transcribe & Summarize") and (yt_link or audio_file):
Â  Â  Â  Â  audio_path = None
Â  Â  Â  Â  preacher = "Unknown"; title = "Untitled Sermon"
Â  Â  Â  Â  cleanup_path = None # Path to delete later

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  with st.spinner("Processing audio..."):
Â  Â  Â  Â  Â  Â  Â  Â  if yt_link:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Validate URL format roughly
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not yt_link.startswith(("http://", "https://")):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise ValueError("Invalid YouTube URL provided.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Get duration without full download first
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with yt_dlp.YoutubeDL({"quiet": True, "noprogress": True}) as ydl:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  info = ydl.extract_info(yt_link, download=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  duration = info.get("duration")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add a reasonable limit, e.g., 20 mins (1200 seconds)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if duration and duration > 1200:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"âš ï¸ Warning: Video is longer than 20 minutes ({duration}s). Transcription might be slow or fail.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  preacher = info.get("uploader", "Unknown") or "Unknown"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title = info.get("title", "Untitled Sermon") or "Untitled Sermon"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Download the audio
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_path, _, _ = download_youtube_audio(yt_link)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cleanup_path = audio_path # Mark for deletion

Â  Â  Â  Â  Â  Â  Â  Â  elif audio_file:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Save uploaded file to a temporary path
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  suffix = os.path.splitext(audio_file.name)[1].lower() or ".tmp"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp_audio:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_audio.write(audio_file.getvalue())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_path = temp_audio.name
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cleanup_path = audio_path # Mark for deletion
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title = os.path.splitext(audio_file.name)[0] # Use filename as title

Â  Â  Â  Â  Â  Â  Â  Â  if not audio_path:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please provide a YouTube link or upload an audio file.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  Â  Â  with st.spinner("Transcribing audio... (This may take a few minutes)"):
Â  Â  Â  Â  Â  Â  Â  Â  # Load the base model (faster, less accurate)
Â  Â  Â  Â  Â  Â  Â  Â  # Consider 'small' or 'medium' for better accuracy if performance allows
Â  Â  Â  Â  Â  Â  Â  Â  model = whisper.load_model("base")Â 
Â  Â  Â  Â  Â  Â  Â  Â  transcription = None
Â  Â  Â  Â  Â  Â  Â  Â  try:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcription = model.transcribe(audio_path, fp16=False) # fp16=False for CPU
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e_transcribe:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Initial transcription failed ({e_transcribe}), trying conversion to WAV...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wav_path = _convert_to_wav_if_needed(audio_path)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cleanup_path = wav_path # Now delete the wav file instead
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcription = model.transcribe(wav_path, fp16=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e_convert:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise Exception(f"Transcription failed even after conversion: {e_convert}")

Â  Â  Â  Â  Â  Â  Â  Â  if not transcription or not transcription.get("text"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise Exception("Transcription failed or produced empty text.")

Â  Â  Â  Â  Â  Â  Â  Â  transcript_text = transcription["text"].strip()
Â  Â  Â  Â  Â  Â  Â  Â  st.success("âœ… Transcription complete.")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ğŸ“ Transcript")
Â  Â  Â  Â  Â  Â  Â  Â  st.text_area("Full Transcript", transcript_text, height=300)

Â  Â  Â  Â  Â  Â  with st.spinner("Generating summary..."):
Â  Â  Â  Â  Â  Â  Â  Â  # Limit summary input to avoid excessive token usage
Â  Â  Â  Â  Â  Â  Â  Â  summary_input_limit = 4000Â 
Â  Â  Â  Â  Â  Â  Â  Â  short_transcript = transcript_text[:summary_input_limit]Â 
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  summary_prompt = f"""You are a sermon summarizer. From the transcript below, provide a concise summary including:
- **Main Topic/Theme:** (Identify the core subject)
- **Key Bible Verses Referenced:** (List primary scriptures mentioned)
- **Main Takeaways:** (Bullet points of key messages or lessons)
- **Potential Reflection Questions:** (2-3 questions for the listener to consider)

Preacher: {preacher}
Title: {title}
Transcript Snippet (first ~{summary_input_limit} characters):
{short_transcript}"""

Â  Â  Â  Â  Â  Â  Â  Â  summary = ask_gpt_conversation(summary_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ğŸ§  Sermon Summary")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(summary)

Â  Â  Â  Â  Â  Â  Â  Â  # Save results
Â  Â  Â  Â  Â  Â  Â  Â  os.makedirs("sermon_journal", exist_ok=True)
Â  Â  Â  Â  Â  Â  Â  Â  ts = datetime.now().strftime("%Y%m%d_%H%M%S")
Â  Â  Â  Â  Â  Â  Â  Â  base_filename = re.sub(r'[\\/*?:"<>|]', "", title)[:50] # Sanitize title for filename
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  transcript_filename = os.path.join("sermon_journal", f"transcript_{base_filename}_{ts}.txt")
Â  Â  Â  Â  Â  Â  Â  Â  summary_filename = os.path.join("sermon_journal", f"summary_{base_filename}_{ts}.txt")
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  with open(transcript_filename, "w", encoding="utf-8") as f: f.write(transcript_text)
Â  Â  Â  Â  Â  Â  Â  Â  with open(summary_filename, "w", encoding="utf-8") as f: f.write(summary)
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Saved transcript and summary to `{transcript_filename}` and `{summary_filename}`.")

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"âŒ An error occurred: {e}")
Â  Â  Â  Â  finally:
Â Â  Â  Â  Â  Â  Â  # Clean up temporary audio file
Â  Â  Â  Â  Â  Â  if cleanup_path and os.path.exists(cleanup_path):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  os.remove(cleanup_path)
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e_clean:
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Could not delete temporary file {cleanup_path}: {e_clean}")

# ================================================================
# SIMPLE STUDY PLAN
# ================================================================
def run_study_plan():
Â  Â  st.subheader("ğŸ“… Personalized Bible Study Plan")
Â  Â  goal = st.text_input("Study goal (e.g., 'Grow in faith', 'Understand forgiveness'):")
Â  Â  duration = st.slider("How many days do you want your plan to last?", 7, 60, 14)
Â  Â  focus = st.text_input("Focus area (e.g., 'Parables', 'Life of Paul', leave blank for general):")
Â  Â  level_plan = st.selectbox("Your Bible knowledge level:", ["Beginner", "Intermediate", "Advanced"], key="plan_level")
Â  Â  include_reflections = st.checkbox("Include daily reflection questions?", True)

Â  Â  if st.button("Generate Study Plan") and goal:
Â  Â  Â  Â  with st.spinner("âœï¸ Creating your personalized study plan..."):
Â  Â  Â  Â  Â  Â  prompt = f"""You are a mature Bible mentor creating a detailed, Scripture-based daily study plan.
**Parameters:**
- Goal: {goal}
- Duration: {duration} days
- Focus area: {focus if focus else 'Based on Goal'}
- Knowledge level: {level_plan}

**Instructions:**
Design a day-by-day Bible study plan. For each day:
- **Day #:**
- **Theme/Title:** A concise theme for the day.
- **Reading:** Suggest 1â€“2 specific Bible passages (e.g., John 3:1-16).
- **Summary:** Explain the passage's meaning and relevance in 3-5 sentences, tailored to the '{level_plan}' level.
- **Connection:** Include 1 cross-reference verse and briefly explain its connection.
- **Application:** Provide a practical life application point or takeaway.
{'- **Reflection:** Add 1 thoughtful reflection question for journaling.' if include_reflections else ''}

Format clearly for each day. End with a brief closing paragraph encouraging consistency. Tone should be pastoral, warm, and theologically sound. Ensure passages logically progress towards the goal."""
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  plan = ask_gpt_conversation(prompt)
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ğŸ“˜ Your Study Plan")
Â  Â  Â  Â  Â  Â  Â  Â  st.text_area("Generated Plan", plan, height=600) # Added label
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Save the plan
Â  Â  Â  Â  Â  Â  Â  Â  os.makedirs("study_plans", exist_ok=True)
Â  Â  Â  Â  Â  Â  Â  Â  timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
Â  Â  Â  Â  Â  Â  Â  Â  # Create a safe filename from the goal
Â  Â  Â  Â  Â  Â  Â  Â  safe_goal = re.sub(r'[\\/*?:"<>|]', "", goal)[:30]
Â  Â  Â  Â  Â  Â  Â  Â  file_path = os.path.join("study_plans", f"study_plan_{safe_goal}_{timestamp}.txt")
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  with open(file_path, "w", encoding="utf-8") as f: f.write(plan)
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"âœ… Study plan saved to `{file_path}`.")
Â  Â  Â  Â  Â  Â  except Exception as e:Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Error generating study plan: {e}")

# ================================================================
# VERSE OF THE DAY, PRAYER STARTER, FAST DEVOTIONAL, SMALL GROUP
# ================================================================
def run_verse_of_the_day():
Â  Â  st.subheader("ğŸŒ… Verse of the Day")
Â  Â  # Expanded list of commonly quoted books
Â  Â  books = ["Genesis", "Exodus", "Leviticus", "Numbers", "Deuteronomy",Â 
Â Â  Â  Â  Â  Â  Â  "Joshua", "Judges", "Ruth", "1 Samuel", "2 Samuel",Â 
Â Â  Â  Â  Â  Â  Â  "1 Kings", "2 Kings", "1 Chronicles", "2 Chronicles",Â 
Â Â  Â  Â  Â  Â  Â  "Ezra", "Nehemiah", "Esther", "Job", "Psalms", "Proverbs",Â 
Â Â  Â  Â  Â  Â  Â  "Ecclesiastes", "Song of Solomon", "Isaiah", "Jeremiah",Â 
Â Â  Â  Â  Â  Â  Â  "Lamentations", "Ezekiel", "Daniel", "Hosea", "Joel", "Amos",Â 
Â Â  Â  Â  Â  Â  Â  "Obadiah", "Jonah", "Micah", "Nahum", "Habakkuk", "Zephaniah",Â 
Â Â  Â  Â  Â  Â  Â  "Haggai", "Zechariah", "Malachi",Â 
Â Â  Â  Â  Â  Â  Â  "Matthew", "Mark", "Luke", "John", "Acts", "Romans",Â 
Â Â  Â  Â  Â  Â  Â  "1 Corinthians", "2 Corinthians", "Galatians", "Ephesians",Â 
Â Â  Â  Â  Â  Â  Â  "Philippians", "Colossians", "1 Thessalonians", "2 Thessalonians",Â 
Â Â  Â  Â  Â  Â  Â  "1 Timothy", "2 Timothy", "Titus", "Philemon", "Hebrews",Â 
Â Â  Â  Â  Â  Â  Â  "James", "1 Peter", "2 Peter", "1 John", "2 John", "3 John",Â 
Â Â  Â  Â  Â  Â  Â  "Jude", "Revelation"]
Â  Â  try:
Â  Â  Â  Â  # Generate a reference - consider API/library for valid chapter/verse counts later
Â  Â  Â  Â  book = random.choice(books)
Â  Â  Â  Â  # Simplified chapter/verse generation for now
Â  Â  Â  Â  chapter = random.randint(1, 5) # Assume at least 5 chapters
Â  Â  Â  Â  verse = random.randint(1, 10) # Assume at least 10 verses
Â  Â  Â  Â  ref = f"{book} {chapter}:{verse}"

Â  Â  Â  Â  with st.spinner("Fetching Verse of the Day..."):
Â  Â  Â  Â  Â  Â  text = fetch_bible_verse(ref, "web") # Default to WEB translation
Â  Â  Â  Â  Â  Â  st.success(f"**{ref} (WEB)**\n\n> {text}") # Use blockquote

Â  Â  Â  Â  Â  Â  reflection_prompt = f"Offer a brief (2-3 sentences), warm, and practical reflection on this Bible verse, focusing on one simple takeaway: '{text}' ({ref})"
Â  Â  Â  Â  Â  Â  reflection = ask_gpt_conversation(reflection_prompt)
Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ’¬ Reflection & Takeaway:**")
Â  Â  Â  Â  Â  Â  st.write(reflection)

Â  Â  except Exception as e:Â 
Â  Â  Â  Â  # Handle cases where the random verse might be invalid
Â  Â  Â  Â  st.error(f"Could not fetch Verse of the Day ({ref}): {e}")Â 

def run_prayer_starter():
Â  Â  st.subheader("ğŸ™ Prayer Starter")
Â  Â  theme = st.text_input("What's on your heart? (e.g., gratitude, anxiety, guidance, forgiveness):")
Â  Â  if st.button("Generate Prayer Starter") and theme:
Â  Â  Â  Â  with st.spinner("Crafting a prayer..."):
Â  Â  Â  Â  Â  Â  prayer = ask_gpt_conversation(f"Write a short (3-5 sentences), theologically faithful prayer starter focused on '{theme}'. Address God reverently (e.g., 'Heavenly Father', 'Lord Jesus') and base it on biblical truths. Avoid clichÃ©s.")
Â  Â  Â  Â  Â  Â  st.text_area("Your Prayer Starter:", prayer, height=200)

def run_fast_devotional():
Â  Â  st.subheader("âš¡ Fast Devotional")
Â  Â  topic = st.text_input("Devotional Topic (e.g., hope, perseverance, love, faith):")
Â  Â  if st.button("Generate Fast Devotional") and topic:
Â  Â  Â  Â  with st.spinner("Writing devotional..."):
Â  Â  Â  Â  Â  Â  devo = ask_gpt_conversation(f"Compose a short devotional (approx. 150-200 words) on the topic of '{topic}'. Include one primary Bible verse, 1-2 related cross-references, a brief explanation connecting them to the topic, and one practical challenge or encouragement for today.")
Â  Â  Â  Â  Â  Â  st.text_area(f"Devotional on {topic}:", devo, height=350)

def run_small_group_generator():
Â  Â  st.subheader("ğŸ‘¥ Small Group Guide Generator")
Â  Â  passage = st.text_input("Bible Passage for Discussion (e.g., James 1:2-8):")
Â  Â  group_size = st.slider("Approximate Group Size:", 2, 15, 5) # Optional context
Â Â  Â 
Â  Â  if st.button("Create Discussion Guide") and passage:
Â  Â  Â  Â  with st.spinner("Generating guide..."):
Â  Â  Â  Â  Â  Â  try:Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Fetch text to provide context to the AI
Â  Â  Â  Â  Â  Â  Â  Â  text = fetch_bible_verse(passage, "web")Â 
Â  Â  Â  Â  Â  Â  Â  Â  context_text = f"The passage is {passage}:\n\n> {text}\n\n"
Â  Â  Â  Â  Â  Â  except Exception:Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Could not fetch text for {passage}, generating questions based on reference only.")
Â  Â  Â  Â  Â  Â  Â  Â  context_text = f"The passage is {passage}.\n\n"

Â  Â  Â  Â  Â  Â  guide_prompt = f"""Create a concise small group discussion guide for a group of about {group_size} people based on the following passage:
{context_text}
**Include:**
- **Opener:** One brief icebreaker question related to the theme.
- **Discussion Questions:** 3-4 thoughtful questions exploring Observation (What does it say?), Interpretation (What does it mean?), and Application (How does it apply to our lives?).
- **Key Truth:** One central takeaway message from the passage.
- **Closing:** A short closing prayer prompt or challenge."""
Â  Â  Â  Â  Â  Â  guide = ask_gpt_conversation(guide_prompt)
Â  Â  Â  Â  Â  Â  st.text_area(f"Discussion Guide for {passage}:", guide, height=500)

# ================================================================
# LEARN MODULE (NEW, PERSONALIZED WORKFLOW)
# ================================================================
def _learn_extract_json_any(response_text: str):
Â  Â  """Robustly extracts JSON object or array from a string."""
Â  Â  if not response_text: return None # Handle empty input
Â  Â  # Prioritize fenced code blocks
Â  Â  match = re.search(r"```json\s*([\s\S]*?)\s*```", response_text)
Â  Â  if match:
Â  Â  Â  Â  json_str = match.group(1)
Â  Â  else:
Â  Â  Â  Â  # Fallback: Find first '{' or '[' and try to parse from there
Â  Â  Â  Â  start_index = -1
Â  Â  Â  Â  first_brace = response_text.find('{')
Â  Â  Â  Â  first_bracket = response_text.find('[')
Â Â  Â  Â  Â 
Â  Â  Â  Â  if first_brace != -1 and (first_bracket == -1 or first_brace < first_bracket):
Â  Â  Â  Â  Â  Â  start_index = first_brace
Â  Â  Â  Â  Â  Â  end_char = '}'
Â  Â  Â  Â  elif first_bracket != -1:
Â  Â  Â  Â  Â  Â  start_index = first_bracket
Â  Â  Â  Â  Â  Â  end_char = ']'
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  if start_index != -1:
Â  Â  Â  Â  Â  Â  # Try to find matching closing bracket/brace - basic nesting support
Â  Â  Â  Â  Â  Â  open_count = 0
Â  Â  Â  Â  Â  Â  end_index = -1
Â  Â  Â  Â  Â  Â  for i, char in enumerate(response_text[start_index:]):
Â  Â  Â  Â  Â  Â  Â  Â  if char == response_text[start_index]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  open_count += 1
Â  Â  Â  Â  Â  Â  Â  Â  elif char == end_char:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  open_count -= 1
Â  Â  Â  Â  Â  Â  Â  Â  if open_count == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  end_index = start_index + i + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  if end_index != -1:
Â  Â  Â  Â  Â  Â  Â  Â  json_str = response_text[start_index:end_index]
Â  Â  Â  Â  Â  Â  else: # Fallback if matching bracket not found
Â  Â  Â  Â  Â  Â  Â  Â  json_str = response_text[start_index:]Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.error("No JSON object or array found in AI response.")
Â  Â  Â  Â  Â  Â  return None

Â  Â  try:
Â  Â  Â  Â  return json.loads(json_str)
Â  Â  except json.JSONDecodeError as e:
Â  Â  Â  Â  st.error(f"Failed to decode JSON from AI response: {e}\nRaw content was: {json_str[:500]}...") # Show snippet
Â  Â  Â  Â  return None

# ============================
# LEARN MODULE SUPPORT HELPERS
# ============================
TOKENS_BY_TIME = {"15 minutes": 1800, "30 minutes": 3000, "45 minutes": 4000} # Rough estimates

# --- NEW SYSTEM PROMPT FOR LEARN MODULE ---
LEARN_MODULE_SYSTEM_PROMPT = """
You are a master theologian and pastoral teacher, an expert in creating biblically-dense, theologically sound, and highly structured curriculum.
Your primary goal is to guide the user to a deep, accurate, and practical understanding of Scripture.
- **Theology:** You adhere to a high view of Scripture (inerrant, infallible, and sufficient).
- **Tone:** Pastoral, encouraging, clear, and authoritative.
- **Clarity:** You MUST define complex theological terms (e.g., "justification," "sanctification") in simple ways, especially for lower-level learners.
- **Output:** You respond ONLY with the valid JSON structure requested. Do not add any conversational text outside the JSON.
"""

def ask_gpt_json(prompt: str, max_tokens: int = 4000):
Â  Â  """Makes a call to the OpenAI API expecting a JSON response."""
Â  Â  try:
Â  Â  Â  Â  resp = client.chat.completions.create(
Â  Â  Â  Â  Â  Â  model=MODEL,
Â  Â  Â  Â  Â  Â  messages=[
Â  Â  Â  Â  Â  Â  Â  Â  {"role": "system", "content": LEARN_MODULE_SYSTEM_PROMPT}, # <-- MODIFIED
Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt}
Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  max_tokens=max_tokens,
Â  Â  Â  Â  Â  Â  temperature=0.2, # Lower temperature for more deterministic JSON output
Â  Â  Â  Â  Â  Â  response_format={"type": "json_object"} # Use JSON mode if available
Â  Â  Â  Â  )
Â  Â  Â  Â  return resp.choices[0].message.content
Â  Â  except Exception as e: # Catch potential API errors (like invalid request due to JSON mode)
Â  Â  Â  Â  try: # Fallback without JSON mode
Â  Â  Â  Â  Â  Â  st.warning(f"JSON mode failed ({e}), attempting standard request...")
Â  Â  Â  Â  Â  Â  resp = client.chat.completions.create(
Â  Â  Â  Â  Â  Â  Â  Â  model=MODEL,
Â  Â  Â  Â  Â  Â  Â  Â  messages=[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "system", "content": LEARN_MODULE_SYSTEM_PROMPT + "\nRespond ONLY with valid JSON wrapped in ```json ``` tags."}, # <-- MODIFIED
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt}
Â  Â  Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  Â  Â  max_tokens=max_tokens,
Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.2
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  return resp.choices[0].message.content
Â  Â  Â  Â  except Exception as e2:
Â  Â  Â  Â  Â  Â  st.error(f"GPT JSON call failed completely: {e2}")
Â  Â  Â  Â  Â  Â  return None


def _answers_match(user_answer, correct_answer, question_type="text") -> bool:
Â  Â  """Flexible answer matching for quizzes, including fuzzy matching for text."""
Â  Â  if user_answer is None or correct_answer is None: return False
Â Â  Â 
Â  Â  user_ans_str = str(user_answer).strip()
Â  Â  correct_ans_str = str(correct_answer).strip()
Â Â  Â 
Â  Â  # Exact match needed for multiple choice and true/false
Â  Â  if question_type == 'multiple_choice' or question_type == 'true_false':
Â  Â  Â  Â  return user_ans_str.lower() == correct_ans_str.lower()
Â Â  Â 
Â  Â  # Use fuzzy matching for fill-in-the-blank (tolerant of typos)
Â  Â  # fuzz.ratio calculates similarity from 0 to 100
Â  Â  similarity_ratio = fuzz.ratio(user_ans_str.lower(), correct_ans_str.lower())
Â  Â  # Adjust threshold as needed - 85 allows for minor errors
Â  Â  return similarity_ratio >= 85Â 

def summarize_lesson_content(lesson_data: dict) -> str:
Â  Â  """Summarizes lesson content for context memory."""
Â  Â  text_content = " ".join([sec.get('content', '') for sec in lesson_data.get('lesson_content_sections', []) if sec.get('type') == 'text'])
Â  Â  if not text_content: return "No textual content available for summary."
Â  Â  # Limit length passed to summarizer
Â  Â  prompt = f"Summarize the key topic of the following Bible lesson text in one concise sentence (less than 20 words): {text_content[:2000]}"
Â  Â  try:
Â  Â  Â  Â  # Use a faster/cheaper model potentially? For now, stick with primary.
Â  Â  Â  Â  resp = client.chat.completions.create(model=MODEL, messages=[{"role": "user", "content": prompt}], max_tokens=60, temperature=0.1)Â 
Â  Â  Â  Â  return resp.choices[0].message.content.strip()
Â  Â  except Exception as e:
Â  Â  Â  Â  st.warning(f"Could not generate lesson summary: {e}")
Â  Â  Â  Â  return lesson_data.get("lesson_title", "Summary unavailable.") # Fallback

# -------------------------
# PROMPTS
# -------------------------
def create_full_learning_plan_prompt(form_data: dict) -> str:
Â  Â  """Creates the master prompt to generate the entire curriculum."""
Â  Â  pacing_to_lessons_per_level = {
Â  Â  Â  Â  "A quick, high-level overview": 1,
Â  Â  Â  Â  "A steady, detailed study": 2,
Â  Â  Â  Â  "A deep, comprehensive dive": 3
Â  Â  }
Â  Â  num_lessons_per_level = pacing_to_lessons_per_level.get(form_data['pacing'], 2) # Default to steady

Â  Â  return f"""
You are an expert theologian and personalized curriculum designer creating a Bible study plan.
User Profile:
- Topics of Interest: {form_data['topics']}
- Current Knowledge: {form_data['knowledge_level']} (Derived from diagnostic)
- Learning Goal: {", ".join(form_data['objectives'])}
- Common Struggles: {", ".join(form_data['struggles'])}
- Preferred Learning Style: {form_data['learning_style']}
- Desired Pacing: {form_data['pacing']}
- Time Commitment per Lesson: {form_data['time_commitment']}

Task: Design a complete Bible study curriculum plan based on this profile.
1. Create a personalized `plan_title`.
2. Write a brief, encouraging `introduction`.
3. Determine the appropriate number of levels based on the pacing (`quick`: 2-3 levels, `steady`: 3-5 levels, `deep`: 5-7 levels).
4. For each level, create a concise `name` and `topic` that flows logically towards the user's goals.
5. For each level, set `num_lessons` based on the user's 'Desired Pacing' (1 for 'quick', 2 for 'steady', 3 for 'deep').

Output ONLY a single, valid JSON object with keys "plan_title", "introduction", and "levels" (a list of objects, each with "name", "topic", and "num_lessons").
Example level object: {{"name": "Level 1: Title", "topic": "Brief topic description", "num_lessons": {num_lessons_per_level}}}
"""

# ================================================================
# <<< NEW FUNCTION >>>
# create_remediation_question_prompt
# ================================================================
def create_remediation_question_prompt(original_question: dict, user_answer: str) -> str:
Â  Â  """
Â  Â  Generates a prompt to create a new question in a different format
Â  Â  for a concept the user failed.
Â  Â  """
Â  Â  q_data = original_question
Â  Â  original_type = q_data.get('question_type', 'unknown')
Â Â  Â 
Â  Â  # Determine which new formats are allowed
Â  Â  possible_formats = ["multiple_choice", "true_false", "fill_in_the_blank"]
Â  Â  # Remove the original format to ensure it's different
Â  Â  if original_type in possible_formats:
Â  Â  Â  Â  possible_formats.remove(original_type)
Â Â  Â 
Â  Â  # Ensure we have at least one format
Â  Â  if not possible_formats:
Â  Â  Â  Â  possible_formats = ["multiple_choice"] # Default fallback

Â  Â  new_format_options = ", ".join(possible_formats)

Â  Â  return f"""
You are an expert Bible quiz designer. A student needs a remediation question.
They were asked the following question:
- **Original Question:** {q_data.get('question')}
- **Original Type:** {original_type}
- **Their Incorrect Answer:** {user_answer}
- **The Correct Answer:** {q_data.get('correct_answer')}
- **Core Concept (from verse):** {q_data.get('biblical_reference')}

**Your Task:**
Create one new question that tests the **exact same core concept** but in a **different format**.
- The new question's format MUST be one of: [{new_format_options}].
- The question must be clear and test the same knowledge.

**Output Format:**
Respond ONLY with a single, valid JSON object with these keys:
- "question": (The new question text)
- "question_type": (The new format, e.g., "multiple_choice")
- "correct_answer": (The correct answer string for the new question)
- "biblical_reference": "{q_data.get('biblical_reference')}"
- "options": (A list of 4 strings, ONLY if `question_type` is "multiple_choice". Must include the correct answer.)
"""

# ================================================================
# <<< MODIFIED FUNCTION >>>
# create_lesson_prompt
# ================================================================
def create_lesson_prompt(level_topic: str, lesson_number: int, total_lessons_in_level: int, form_data: dict, previous_lesson_summary: str = None, previous_struggles: str = None) -> str:
Â  Â  """Generates a robust, theologically sound prompt for a single lesson."""
Â Â  Â 
Â  Â  # --- Context Clauses ---
Â  Â  context_clause_lesson = f" This lesson must logically follow the previous one, which covered: '{previous_lesson_summary}'." if previous_lesson_summary else ""
Â  Â  # <<< NEW >>> Add struggle context
Â  Â  context_clause_struggle = (
Â  Â  Â  Â  f" **Adaptive Learning Note:** This user has previously struggled with these topics: [{previous_struggles}]. "
Â  Â  Â  Â  "If relevant to this lesson, you **MUST** add a 'Review' section at the beginning to briefly re-explain one of those concepts before teaching new material."
Â  Â  ) if previous_struggles else ""

Â  Â  knowledge_level = form_data['knowledge_level']
Â  Â  learning_style = form_data['learning_style']
Â  Â  time_commitment = form_data['time_commitment']

Â  Â  # --- Define Level Instructions ---
Â  Â  level_instructions = ""
Â  Â  if knowledge_level == "Just starting out":
Â  Â  Â  Â  level_instructions = "Focus on the core narrative and clear application. Define ALL theological terms (e.g., 'grace', 'justification', 'redemption'). Assume no prior knowledge."
Â  Â  elif knowledge_level == "I know the main stories":
Â  Â  Â  Â  level_instructions = "Connect the text to broader biblical themes (e.g., covenant, kingdom). Introduce and define one or two key theological concepts per lesson."
Â  Â  else: # "I'm comfortable with deeper concepts"
Â  Â  Â  Â  level_instructions = "Include historical context, connections to original languages (e.g., 'the Greek word for love here is *agape*...'), and deeper doctrinal synthesis. Do not shy away from complex ideas."

Â  Â  # --- Enhanced Style Instructions ---
Â  Â  style_instructions = ""
Â  Â  if learning_style == "analytical":
Â  Â  Â  Â  style_instructions = "Your teaching method is **Analytical**. Focus on theological terms, logical structure, and doctrinal categories. Use bullet points. Ask 'WHAT' does this text teach us about God, sin, and salvation?"
Â  Â  elif learning_style == "storytelling":
Â  Â  Â  Â  style_instructions = "Your teaching method is **Narrative Illustration**. Focus on the *theological principles* within the story. Use narrative examples (from the text or modern life) to *illustrate* these principles. Ask 'WHY' is this story in the Bible and what truth does it reveal?"
Â  Â  elif learning_style == "practical":
Â  Â  Â  Â  style_instructions = "Your teaching method is **Practical Application**. State the biblical principle clearly and briefly. Then, spend *most* of the section on 'How to use this' and 'What this looks like today' with concrete, actionable steps."
Â  Â  elif learning_style == "reflective":
Â  Â  Â  Â  style_instructions = "Your teaching method is **Introspective**. Focus on internal transformation. Ask probing, *italicized, bolded questions* directly within the text to make the user connect the doctrine to their own heart and motivations."

Â  Â  # --- Define the *exact* section structure ---
Â  Â  # <<< NEW >>> Added {{REVIEW_SECTION_IF_NEEDED}} placeholder
Â  Â  section_structure_instructions = ""
Â  Â  if time_commitment == "15 minutes":
Â  Â  Â  Â  section_structure_instructions = """
Â  Â  Â  Â  {{REVIEW_SECTION_IF_NEEDED}}
Â  Â  Â  Â  - A 'text' section with the role 'Introduction'.
Â  Â  Â  Â  - A 'text' section with the role 'Core Teaching & Application'.
Â  Â  Â  Â  - A 'knowledge_check' section testing the 'Core Teaching'.
Â  Â  Â  Â  """
Â  Â  elif time_commitment == "30 minutes":
Â  Â  Â  Â  section_structure_instructions = """
Â  Â  Â  Â  {{REVIEW_SECTION_IF_NEEDED}}
Â  Â  Â  Â  - A 'text' section with the role 'Introduction' (State the main topic and passage).
Â  Â  Â  Â  - A 'text' section with the role 'Exposition' (Teach the theological principles *from* the passage).
Â  Â  Â  Â  - A 'knowledge_check' section testing the 'Exposition'.
Â  Â  Â  Â  - A 'text' section with the role 'Application' (Provide a 'So what?' for daily life).
Â  Â  Â  Â  - A 'knowledge_check' section testing the 'Application'.
Â  Â  Â  Â  """
Â  Â  else: # 45 minutes
Â  Â  Â  Â  section_structure_instructions = """
Â  Â  Â  Â  {{REVIEW_SECTION_IF_NEEDED}}
Â  Â  Â  Â  - A 'text' section with the role 'Introduction' (A compelling hook and the main theological question).
Â  Â  Â  Â  - A 'text' section with the role 'Exposition' (A deep dive into the *theological principles* of the passage).
Â  Â  Â  Â  - A 'knowledge_check' section testing the 'Exposition'.
Â  Â  Â  Â  - A 'text' section with the role 'Theological Connection' (Connect these principles to another part of the Bible or a core doctrine).
Â  Â  Â  Â  - A 'knowledge_check' section testing the 'Theological Connection'.
Â  Â  Â  Â  - A 'text' section with the role 'Practical Application' (A clear, actionable takeaway for daily life).
Â  Â  Â  Â  - A 'text' section with the role 'Guided Reflection' (A closing prayer prompt or reflective questions).
Â  Â  Â  Â  """
Â Â  Â 
Â  Â  # <<< NEW >>> Dynamically insert the review block based on struggles
Â  Â  review_block = ""
Â  Â  if previous_struggles:
Â  Â  Â  Â  review_block = f"- A 'text' section with the role 'Review'. (Content: Briefly re-explain a concept from: {previous_struggles}, as a warm-up.)"
Â Â  Â 
Â  Â  # Replace the placeholder with the dynamic block
Â  Â  section_structure_instructions = section_structure_instructions.replace("{{REVIEW_SECTION_IF_NEEDED}}", review_block)


Â  Â  return f"""
You are a master theologian creating Lesson {lesson_number}/{total_lessons_in_level} on the topic of "{level_topic}".
{context_clause_lesson}
{context_clause_struggle}

**User Profile:**
- Knowledge Level: {knowledge_level}
- Learning Style: {learning_style}

**CRITICAL INSTRUCTIONS:**
1.Â  **Core Teaching Philosophy:** Your goal is to *teach theology* (what is true about God) and *doctrine* (what we believe) that is *derived from* the biblical text. **DO NOT simply paraphrase or summarize the plot of the Bible passage.** Extract the *principles* from the story and teach those principles.
2.Â  **Lesson Title:** Create a **unique and specific** `lesson_title` for this lesson. **DO NOT** just repeat the overall level topic ("{level_topic}").
3.Â  **JSON Structure:** You must generate a JSON object with keys "lesson_title", "lesson_content_sections", and "summary_points".
4.Â  **Lesson Content:** The "lesson_content_sections" MUST be a list of objects. You will generate *exactly* these sections in this order:
{section_structure_instructions}
5.Â  **Style is Primary:** The *most important* instruction is to follow the user's `learning_style`. Apply this method to all 'text' sections: {style_instructions}
6.Â  **Theological Depth:** All 'text' sections MUST be theologically sound, biblically dense (citing specific passages like John 3:16), and tailored to the user's `knowledge_level`: {level_instructions}

7.Â  **CRITICAL TEXT SECTION REQUIREMENT:** Every 'text' section **MUST** include a **`role`** key (e.g., 'Introduction', 'Exposition', 'Application') and a **`content`** key (string). Additionally, any 'text' section that directly references a single, primary Bible verse (e.g., John 3:16 or Romans 12:1) **MUST** include an **`audio_reference`** key (string) containing the exact Bible verse reference (e.g., "John 3:16"). This is for providing audio integration in the app.

8.Â  **CRITICAL KEY REQUIREMENT FOR 'knowledge_check':**
Â  Â  - Every 'knowledge_check' object MUST include *all* of these keys:
Â  Â  - `type`: "knowledge_check"
Â  Â  - `question`: "The question text..."
Â  Â  - `question_type`: "multiple_choice" or "true_false" or "fill_in_the_blank"
Â  Â  - `correct_answer`: "The correct answer string..."
Â  Â  - `biblical_reference`: "The relevant verse, e.g., Genesis 1:1"
Â  Â  - `options`: (A list of 4 strings, ONLY if `question_type` is "multiple_choice")
Â  Â  - **LOGIC CHECK:** The `question` text MUST match the `question_type`.
Â  Â  - Â  (Example: A 'true_false' question must be a statement that can be answered True or False, like "True or False: Genesis 3:15 is the first promise of redemption.")
Â  Â  - Â  (Example: A 'multiple_choice' question must be a question with a clear answer in the options.)
Â  Â  - Â  (Example: DO NOT create a 'true_false' question that asks "How..." or "Why...")
Â  Â  - **Failure to follow these rules will fail the lesson.**

Output ONLY the valid JSON object.
"""

def create_level_quiz_prompt(level_topic: str, lesson_summaries: list, level_name: str) -> str:
Â  Â  """Generates prompt for creating the end-of-level quiz."""
Â  Â  summaries_text = "\n".join(f"- {s}" for i, s in enumerate(lesson_summaries) if s) # Filter empty summaries
Â  Â  return f"""
You are a Bible teacher creating a 10-question cumulative quiz for the level titled "{level_name}" which covers the topic of "{level_topic}".
The lessons covered these key points:
{summaries_text}

**Instructions:** Create a quiz based on the *theological concepts, scriptural connections, and applications* implied by these lesson summaries.
- Generate exactly 10 questions.
- Mix question types: 'multiple_choice', 'true_false', 'fill_in_the_blank'.
- Each question object MUST include: 'question' (string), 'question_type' (string), 'correct_answer' (string).
- For 'multiple_choice', also include 'options' (list of 4 strings, including the correct one).
- For 'true_false', `correct_answer` must be "True" or "False".
- Include a relevant 'biblical_reference' (string) for each question.

**Output Format:**
Output ONLY a valid JSON object with a *single key* named "quiz". The value of "quiz" MUST be the JSON *array* containing the 10 question objects.
Example: {{"quiz": [ {{...question 1...}}, {{...question 2...}} ]}}
"""

# -------------------------
# KNOWLEDGE CHECK & QUIZ UI
# -------------------------
# ================================================================
# <<< FIXED & ENHANCED FUNCTION >>>
# display_knowledge_check_question
# ================================================================
def display_knowledge_check_question(S):
Â  Â  """Displays knowledge check, handles submission, and shows breakdown + remediation question."""
Â Â  Â 
Â  Â  level_data = S["levels"][S["current_level"]]
Â  Â  current_lesson = level_data["lessons"][S["current_lesson_index"]]
Â  Â  # The original question from the lesson plan
Â  Â  q_original = current_lesson["lesson_content_sections"][S["current_section_index"]]

Â  Â  input_key_base = f"kc_{S['current_level']}_{S['current_lesson_index']}_{S['current_section_index']}"

Â  Â  # --- STATE 1: User is answering the *original* question ---
Â  Â  if not S.get("awaiting_remediation"):
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.markdown(f"#### âœ… Knowledge Check")
Â  Â  Â  Â  st.markdown(f"**{q_original.get('question', 'Missing question text.')}**")

Â  Â  Â  Â  user_answer = None
Â  Â  Â  Â  input_key = f"{input_key_base}_original"
Â  Â  Â  Â  q_type = q_original.get('question_type')

Â  Â  Â  Â  if q_type == 'multiple_choice':
Â  Â  Â  Â  Â  Â  user_answer = st.radio("Select your answer:", q_original.get('options', []), key=input_key, index=None)
Â  Â  Â  Â  elif q_type == 'true_false':
Â  Â  Â  Â  Â  Â  user_answer = st.radio("True or False?", ['True', 'False'], key=input_key, index=None)
Â  Â  Â  Â  elif q_type == 'fill_in_the_blank':
Â  Â  Â  Â  Â  Â  user_answer = st.text_input("Fill in the blank:", key=input_key)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.error(f"Unknown question type: {q_type}"); return

Â  Â  Â  Â  if st.button("Submit Answer", key=f"submit_{input_key}"):
Â  Â  Â  Â  Â  Â  if user_answer is None and q_type in ['multiple_choice', 'true_false']:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please select an answer.")
Â  Â  Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  Â  Â  is_correct = _answers_match(user_answer, q_original.get('correct_answer'), q_type)
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if is_correct:
Â  Â  Â  Â  Â  Â  Â  Â  st.success("Correct! Moving on.")
Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] += 1
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # --- INCORRECT ---
Â  Â  Â  Â  Â  Â  Â  Â  # 1. Log the struggle
Â  Â  Â  Â  Â  Â  Â  Â  topic = q_original.get('biblical_reference', 'general_knowledge')
Â  Â  Â  Â  Â  Â  Â  Â  S["struggle_log"][topic] = S["struggle_log"].get(topic, 0) + 1
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  # 2. Set state for remediation loop
Â  Â  Â  Â  Â  Â  Â  Â  S["awaiting_remediation"] = True # Set to True to start loop
Â  Â  Â  Â  Â  Â  Â  Â  S["last_incorrect_answer"] = user_answer
Â  Â  Â  Â  Â  Â  Â  Â  S["remediation_question"] = None # Ensure it's cleared
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â 
Â  Â  Â  Â  # Navigation to go back
Â  Â  Â  Â  if S["current_section_index"] > 0:
Â  Â  Â  Â  Â  Â  if st.button("â¬…ï¸ Previous Section", key=f"prev_sec_kc_{S['current_section_index']}"):
Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] -= 1
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  return

Â  Â  # --- STATE 2: User is in the remediation loop (Breakdown + New Question) ---
Â  Â  if S.get("awaiting_remediation") == True: # State is True (not False, not "completed")
Â  Â  Â  Â  st.error(f"Not quite. The correct answer to the first question was: **{q_original.get('correct_answer')}**")
Â Â  Â  Â  Â 
Â  Â  Â  Â  # --- Display Theological Breakdown ---
Â  Â  Â  Â  if "breakdown_content" not in S: # Generate breakdown only once
Â  Â  Â  Â  Â  Â  reference = q_original.get('biblical_reference', '')
Â  Â  Â  Â  Â  Â  breakdown_data = {"explanation": "Loading...", "verse_text": "", "reference": reference}
Â  Â  Â  Â  Â  Â  if reference:
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Loading Theological Breakdown..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  verse_text = fetch_bible_verse(reference)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  incorrect_ans = S.get("last_incorrect_answer", "their answer")
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # <<< ENHANCED PROMPT >>>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  explanation_prompt = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"You are a pastoral Bible teacher providing a 'Theological Breakdown'. A student was asked: '{q_original.get('question')}' "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"They incorrectly answered: '{incorrect_ans}'. The correct answer is: '{q_original.get('correct_answer')}'. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"The relevant Bible verse is '{reference}', which says: '{verse_text}'. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Your task is to provide a clear, structured breakdown. Respond using these exact bolded headers:\n\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"**1. Theological Principle:** (Briefly explain the *theological principle* taught in '{reference}'.)\n\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"**2. Misunderstanding Analysis:** (Gently explain the *specific error* in thinking that led to the answer '{incorrect_ans}'.)\n\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"**3. Truth Reinforced:** (Clearly explain why '{q_original.get('correct_answer')}' is the correct answer, tying it back to the verse's principle.)"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  explanation = ask_gpt_conversation(explanation_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  breakdown_data["explanation"] = explanation
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  breakdown_data["verse_text"] = verse_text
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  breakdown_data["explanation"] = f"Could not load full breakdown: {e}"
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # Fallback if no verse is provided
Â  Â  Â  Â  Â  Â  Â  Â  explanation = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"**1. Theological Principle:** The question addresses the core concept of '{q_original.get('correct_answer')}'."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"**2. Misunderstanding Analysis:** Your answer '{S.get('last_incorrect_answer', 'their answer')}' was likely incorrect because [AI to infer reasoning]."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"**3. Truth Reinforced:** The answer is '{q_original.get('correct_answer')}' because [AI to infer reasoning]."
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  breakdown_data["explanation"] = explanation
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  S["breakdown_content"] = breakdown_data # Save to state
Â Â  Â  Â  Â 
Â  Â  Â  Â  # Display the breakdown
Â  Â  Â  Â  breakdown = S["breakdown_content"]
Â  Â  Â  Â  with st.expander(f"ğŸ“– Theological Breakdown: {breakdown.get('reference')}", expanded=True):
Â  Â  Â  Â  Â  Â  if breakdown.get("verse_text"):
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**Verse Text ({breakdown.get('reference')}):**\n\n> *{breakdown.get('verse_text')}*")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  st.markdown(f"{breakdown.get('explanation')}") # Explanation now has its own markdown formatting

Â  Â  Â  Â  # --- Display Remediation Question ---
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.markdown("#### ğŸ’¡ Let's Try That Concept Again")

Â  Â  Â  Â  # 2a. Generate the remediation question if it doesn't exist
Â  Â  Â  Â  if not S.get("remediation_question"):
Â  Â  Â  Â  Â  Â  with st.spinner("Preparing a new question..."):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  remediation_prompt = create_remediation_question_prompt(q_original, S.get("last_incorrect_answer", ""))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  q_resp = ask_gpt_json(remediation_prompt, 1000)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  q_data = _learn_extract_json_any(q_resp)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not q_data or 'question' not in q_data:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise Exception("Failed to generate valid remediation question JSON.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["remediation_question"] = q_data
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to display the new question
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return # <<< BUG FIX >>> Add return to stop script
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error generating remediation question: {e}. Moving on.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Abort remediation and show 'Continue'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["awaiting_remediation"] = "completed" # Use "completed" to skip to continue button
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "breakdown_content" in S: del S["breakdown_content"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return # <<< BUG FIX >>> Add return to stop script
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  # 2b. Display and process the remediation question
Â  Â  Â  Â  q_remediation = S.get("remediation_question")
Â  Â  Â  Â  if not q_remediation: return # Safeguard

Â  Â  Â  Â  st.markdown(f"**{q_remediation.get('question')}**")
Â Â  Â  Â  Â 
Â  Â  Â  Â  user_answer_remediation = None
Â  Â  Â  Â  input_key_remediation = f"{input_key_base}_remediation"
Â  Â  Â  Â  q_type_remediation = q_remediation.get('question_type')

Â  Â  Â  Â  if q_type_remediation == 'multiple_choice':
Â  Â  Â  Â  Â  Â  user_answer_remediation = st.radio("Select your answer:", q_remediation.get('options', []), key=input_key_remediation, index=None)
Â  Â  Â  Â  elif q_type_remediation == 'true_false':
Â  Â  Â  Â  Â  Â  user_answer_remediation = st.radio("True or False?", ['True', 'False'], key=input_key_remediation, index=None)
Â  Â  Â  Â  elif q_type_remediation == 'fill_in_the_blank':
Â  Â  Â  Â  Â  Â  user_answer_remediation = st.text_input("Fill in the blank:", key=input_key_remediation)
Â Â  Â  Â  Â 
Â  Â  Â  Â  if st.button("Submit Reworked Answer", key=f"submit_{input_key_remediation}"):
Â  Â  Â  Â  Â  Â  if user_answer_remediation is None and q_type_remediation in ['multiple_choice', 'true_false']:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please select an answer.")
Â  Â  Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  Â  Â  is_correct_remediation = _answers_match(user_answer_remediation, q_remediation.get('correct_answer'), q_type_remediation)
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if is_correct_remediation:
Â  Â  Â  Â  Â  Â  Â  Â  st.success("âœ… Excellent! You've grasped the concept.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ Still not quite. The correct answer was **{q_remediation.get('correct_answer')}**. We'll move on for now, but be sure to review this concept!")
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # Set flag to show 'Continue' button
Â  Â  Â  Â  Â  Â  S["awaiting_remediation"] = "completed"Â 
Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  # --- STATE 3: Remediation is done, show 'Continue' button ---
Â  Â  if S.get("awaiting_remediation") == "completed":
Â  Â  Â  Â  st.markdown("---") # Add a separator
Â  Â  Â  Â  if st.button("Continue Lesson", type="primary", key=f"continue_{input_key_base}"):
Â  Â  Â  Â  Â  Â  # Clear all remediation flags
Â  Â  Â  Â  Â  Â  S["awaiting_remediation"] = False
Â  Â  Â  Â  Â  Â  if "last_incorrect_answer" in S: del S["last_incorrect_answer"]
Â  Â  Â  Â  Â  Â  if "remediation_question" in S: del S["remediation_question"]
Â  Â  Â  Â  Â  Â  if "breakdown_content" in S: del S["breakdown_content"]
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # Move to the next section
Â  Â  Â  Â  Â  Â  S["current_section_index"] += 1
Â  Â  Â  Â  Â  Â  st.rerun()


def run_level_quiz(S):
Â  Â  # --- NEW: Add Back to Syllabus Button ---
Â  Â  if st.button("â¬…ï¸ Back to Syllabus"):
Â  Â  Â  Â  S["quiz_mode"] = False
Â  Â  Â  Â  S["view_mode"] = "dashboard"
Â  Â  Â  Â  # Reset quiz progress
Â  Â  Â  Â  S["current_question_index"] = 0Â 
Â  Â  Â  Â  S["user_score"] = 0
Â  Â  Â  Â  st.rerun()
Â  Â  # --- End of NEW Button ---

Â  Â  level_data = S["levels"][S["current_level"]]
Â  Â  quiz_questions = level_data.get("quiz_questions", [])
Â  Â  q_index = S.get("current_question_index", 0)

Â  Â  st.markdown("### ğŸ“ Final Level Quiz")
Â  Â  if not quiz_questions:Â 
Â  Â  Â  Â  st.warning("Quiz questions not generated yet or generation failed.")
Â  Â  Â  Â  return

Â  Â  if not isinstance(quiz_questions, list):
Â Â  Â  Â  Â  st.error("Quiz data is not in the expected format (list). Please restart the level.")
Â Â  Â  Â  Â  return

Â  Â  total_questions = len(quiz_questions)
Â  Â  if total_questions == 0:
Â Â  Â  Â  Â  st.warning("No quiz questions found for this level.")
Â Â  Â  Â  Â  return
Â  Â  Â  Â  Â 
Â  Â  st.progress(q_index / total_questions) # Use q_index for progress
Â  Â  st.markdown(f"**Score: {S.get('user_score', 0)}/{total_questions}**")

Â  Â  if q_index < total_questions:
Â  Â  Â  Â  q = quiz_questions[q_index]
Â  Â  Â  Â  if not isinstance(q, dict) or 'question' not in q or 'correct_answer' not in q:
Â Â  Â  Â  Â  Â  Â  st.error(f"Error: Invalid question format at index {q_index}. Skipping question.")
Â Â  Â  Â  Â  Â  Â  S["current_question_index"] = q_index + 1
Â Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  st.markdown(f"**Question {q_index + 1}:** {q.get('question', '')}")
Â  Â  Â  Â  user_answer = None
Â  Â  Â  Â  q_key = f"quiz_{S['current_level']}_{q_index}"
Â  Â  Â  Â  q_type = q.get('question_type')

Â  Â  Â  Â  if q_type == 'multiple_choice':
Â  Â  Â  Â  Â  Â  options = q.get('options', [])
Â  Â  Â  Â  Â  Â  if not options: st.error("Error: Multiple choice question has no options."); return
Â  Â  Â  Â  Â  Â  user_answer = st.radio("Answer:", options, key=q_key, index=None)
Â  Â  Â  Â  elif q_type == 'true_false':
Â  Â  Â  Â  Â  Â  user_answer = st.radio("Answer:", ["True", "False"], key=q_key, index=None)
Â  Â  Â  Â  elif q_type == 'fill_in_the_blank':
Â  Â  Â  Â  Â  Â  user_answer = st.text_input("Answer:", key=q_key)
Â  Â  Â  Â  else:
Â Â  Â  Â  Â  Â  Â  st.error(f"Unknown quiz question type: {q_type}"); return

Â  Â  Â  Â  if st.button("Submit Quiz Answer", key=f"submit_{q_key}"):
Â  Â  Â  Â  Â  Â  if user_answer is None and q_type in ['multiple_choice', 'true_false']:
Â Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please select an answer.")
Â Â  Â  Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if _answers_match(user_answer, q.get('correct_answer'), q_type):
Â  Â  Â  Â  Â  Â  Â  Â  st.success("Correct!")
Â  Â  Â  Â  Â  Â  Â  Â  S["user_score"] = S.get("user_score", 0) + 1
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Incorrect. The correct answer was: **{q.get('correct_answer')}**")
Â  Â  Â  Â  Â  Â  Â  Â  # <<< ADAPTIVE LEARNING >>> Log struggle topic from quiz
Â  Â  Â  Â  Â  Â  Â  Â  topic = q.get('biblical_reference', 'general_knowledge')
Â  Â  Â  Â  Â  Â  Â  Â  S["struggle_log"][topic] = S["struggle_log"].get(topic, 0) + 1
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  S["current_question_index"] = q_index + 1
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  else:
Â  Â  Â  Â  # Quiz completed
Â  Â  Â  Â  score = S.get('user_score', 0)
Â  Â  Â  Â  passing_score = total_questions * 0.7Â 
Â  Â  Â  Â  st.success(f"### Quiz Completed! Final Score: {score}/{total_questions}")
Â Â  Â  Â  Â 
Â  Â  Â  Â  if score >= passing_score:
Â  Â  Â  Â  Â  Â  st.balloons()
Â  Â  Â  Â  Â  Â  st.markdown(f"Congratulations! You passed {level_data.get('name','this level')}!")
Â  Â  Â  Â  Â  Â  level_data["quiz_completed"] = True # <-- NEW: Set level complete flag
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if S["current_level"] + 1 < len(S["levels"]):
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Go to Next Level â–¶ï¸"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_level"] += 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_lesson_index"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["quiz_mode"] = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_question_index"] = 0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["user_score"] = 0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard" # <-- NEW: Go back to dashboard
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â Â  Â  Â  Â  Â  Â  Â  Â  S["plan_completed"] = True # <-- NEW: Set plan complete flag
Â Â  Â  Â  Â  Â  Â  Â  Â  st.info("You've completed all levels in this plan!")
Â Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Back to Syllabus"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard" # <-- NEW: Go back to dashboard
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["quiz_mode"] = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.error("You didn't reach the passing score. Please review the lessons and try the quiz again.")
Â  Â  Â  Â  Â  Â  if st.button("Review Lessons"):
Â Â  Â  Â  Â  Â  Â  Â  Â  S["quiz_mode"] = False
Â Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard" # <-- NEW: Go back to dashboard
Â Â  Â  Â  Â  Â  Â  Â  Â  S["current_question_index"] = 0Â 
Â Â  Â  Â  Â  Â  Â  Â  Â  S["user_score"] = 0
Â Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  if st.button("Retake Quiz"):
Â  Â  Â  Â  Â  Â  Â  Â  S["current_question_index"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  S["user_score"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

# ================================================================
# DIAGNOSTIC QUIZ FUNCTION
# ================================================================
def run_diagnostic_quiz():
Â  Â  st.subheader("Quick Bible Knowledge Check")
Â  Â  st.info("Let's figure out the best starting point for you with a few quick questions.")
Â  Â  S_learn = st.session_state.learn_stateÂ 
Â  Â  if 'diag_q_index' not in S_learn:
Â  Â  Â  Â  S_learn['diag_q_index'] = 0
Â  Â  Â  Â  S_learn['diag_score'] = 0
Â  Â  q_index = S_learn['diag_q_index']
Â  Â  if q_index < len(DIAGNOSTIC_QUESTIONS):
Â  Â  Â  Â  q_data = DIAGNOSTIC_QUESTIONS[q_index]
Â  Â  Â  Â  st.markdown(f"**Question {q_index + 1} of {len(DIAGNOSTIC_QUESTIONS)}:**")
Â  Â  Â  Â  st.markdown(f"*{q_data['question']}*")
Â Â  Â  Â  Â 
Â  Â  Â  Â  options = q_data['options']
Â  Â  Â  Â  if "I don't know" not in options: options.append("I don't know")
Â Â  Â  Â  Â 
Â  Â  Â  Â  user_answer = st.radio(
Â  Â  Â  Â  Â  Â  "Select your answer:",
Â  Â  Â  Â  Â  Â  options,
Â  Â  Â  Â  Â  Â  key=f"diag_q_{q_index}",
Â  Â  Â  Â  Â  Â  index=NoneÂ 
Â  Â  Â  Â  )
Â  Â  Â  Â  if st.button("Submit Answer", key=f"diag_submit_{q_index}"):
Â  Â  Â  Â  Â  Â  if user_answer:
Â  Â  Â  Â  Â  Â  Â  Â  if user_answer == q_data['correct']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S_learn['diag_score'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  S_learn['diag_q_index'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please select an answer.")
Â  Â  else:
Â  Â  Â  Â  score = S_learn['diag_score']
Â  Â  Â  Â  total = len(DIAGNOSTIC_QUESTIONS)
Â  Â  Â  Â  knowledge_level = ""
Â  Â  Â  Â  if score / total <= 0.4: # Covers 0 or 1 out of 3
Â  Â  Â  Â  Â  Â  knowledge_level = "Just starting out"
Â  Â  Â  Â  elif score / total <= 0.7: # Covers 2 out of 3
Â  Â  Â  Â  Â  Â  knowledge_level = "I know the main stories"
Â  Â  Â  Â  else: # Covers 3 out of 3
Â  Â  Â  Â  Â  Â  knowledge_level = "I'm comfortable with deeper concepts"

Â  Â  Â  Â  st.success(f"Knowledge check complete! Score: {score}/{total}")
Â  Â  Â  Â  st.info(f"Based on your answers, we'll tailor the plan using the **'{knowledge_level}'** level as a starting point.")
Â Â  Â  Â  Â 
Â  Â  Â  Â  S_learn['derived_knowledge_level'] = knowledge_level
Â  Â  Â  Â  S_learn['diagnostic_complete'] = True
Â  Â  Â  Â  st.rerun()Â 

# ================================================================
# LEARNING PLAN SETUP (QUESTIONNAIRE)
# ================================================================
def run_learn_module_setup():
Â  Â  st.info("Now, let's create a personalized learning plan based on your unique needs.")
Â  Â  derived_knowledge_level = st.session_state.learn_state.get('derived_knowledge_level', "Not determined")
Â  Â  st.markdown(f"**Assessed Knowledge Level:** {derived_knowledge_level}")Â 

Â  Â  with st.form("user_profile_form"):
Â  Â  Â  Â  topics_input = st.text_input("**What topics are on your heart to learn about?** (Separate with commas)", "Understanding grace, The life of David")
Â  Â  Â  Â  objectives_input = st.multiselect("**What do you hope to achieve with this study?**", ["Gain knowledge and understanding", "Find practical life application", "Strengthen my faith", "Prepare to teach others"], default=["Gain knowledge and understanding"])
Â  Â  Â  Â  struggles_input = st.multiselect("**What are some of your common challenges?**", ["Understanding historical context", "Connecting it to my daily life", "Staying consistent", "Dealing with difficult passages"])
Â  Â  Â  Â  learning_style_input = st.selectbox("**Preferred learning style:**", ["Analytical", "Storytelling", "Practical", "Reflective"])
Â  Â  Â  Â  pacing_input = st.select_slider("**How would you like to pace your learning?**", options=["A quick, high-level overview", "A steady, detailed study", "A deep, comprehensive dive"], value="A steady, detailed study")
Â  Â  Â  Â  time_commitment_input = st.selectbox("**How much time can you realistically commit to each lesson?**", ["15 minutes", "30 minutes", "45 minutes"], index=1)
Â Â  Â  Â  Â 
Â  Â  Â  Â  submitted = st.form_submit_button("ğŸš€ Generate My Tailor-Made Plan")
Â Â  Â 
Â  Â  if submitted:
Â  Â  Â  Â  form_data = {
Â  Â  Â  Â  Â  Â  'topics': topics_input,
Â  Â  Â  Â  Â  Â  'knowledge_level': derived_knowledge_level,Â 
Â  Â  Â  Â  Â  Â  'objectives': objectives_input,
Â  Â  Â  Â  Â  Â  'struggles': struggles_input,
Â  Â  Â  Â  Â  Â  'learning_style': learning_style_input.lower(),
Â  Â  Â  Â  Â  Â  'pacing': pacing_input,
Â  Â  Â  Â  Â  Â  'time_commitment': time_commitment_input
Â  Â  Â  Â  }
Â Â  Â  Â  Â 
Â  Â  Â  Â  if not form_data['topics'] or not form_data['objectives']:
Â  Â  Â  Â  Â  Â  st.warning("Please fill out the topics and objectives to generate a plan.")
Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  if form_data['knowledge_level'] == "Not determined":
Â Â  Â  Â  Â  Â  Â  st.error("Knowledge level could not be determined. Please restart.")
Â Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  with st.spinner("Our AI is designing your personalized curriculum..."):
Â  Â  Â  Â  Â  Â  master_prompt = create_full_learning_plan_prompt(form_data)
Â  Â  Â  Â  Â  Â  plan_resp = ask_gpt_json(master_prompt, max_tokens=2500)
Â  Â  Â  Â  Â  Â  plan_data = _learn_extract_json_any(plan_resp) if plan_resp else None
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if plan_data and isinstance(plan_data, dict) and "levels" in plan_data and isinstance(plan_data["levels"], list):
Â  Â  Â  Â  Â  Â  Â  Â  S = st.session_state.learn_state
Â  Â  Â  Â  Â  Â  Â  Â  S.update({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "plan": plan_data, "levels": plan_data["levels"], "form_data": form_data,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "current_level": 0, "current_lesson_index": 0, "current_section_index": 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "quiz_mode": False, "current_question_index": 0, "user_score": 0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "view_mode": "dashboard", # <-- NEW: Set default view
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.error("Failed to generate a valid learning plan from AI response. Please try adjusting your inputs or try again later.")
Â  Â  Â  Â  Â  Â  Â  Â  if plan_resp: st.text_area("Raw AI Response (for debugging):", plan_resp, height=200)

# ================================================================
# LEARN MODULE: DEEP DIVE CHAT (NEW FUNCTION)
# ================================================================
def run_deep_dive_chat(S):
Â  Â  """
Â  Â  Renders an in-lesson chat interface for a "Deep Dive" on a specific section.
Â  Â  Uses the main THEOLOGICAL_SYSTEM_PROMPT.
Â  Â  """
Â  Â  st.markdown("---")
Â  Â  st.subheader("Deep Dive Q&A")
Â  Â  st.info(f"You are asking a question about the section you just read. Your chat history here is temporary.")
Â Â  Â 
Â  Â  # Initialize deep dive history if it doesn't exist
Â  Â  if "deep_dive_history" not in S:
Â  Â  Â  Â  S["deep_dive_history"] = []

Â  Â  # Display the mini-chat history
Â  Â  chat_container = st.container(height=300, border=True)
Â  Â  with chat_container:
Â  Â  Â  Â  if not S["deep_dive_history"]:
Â  Â  Â  Â  Â  Â  st.caption("Ask your question below...")
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  for msg in S["deep_dive_history"]:
Â  Â  Â  Â  Â  Â  who = "âœï¸ Bible GPT" if msg["role"] == "assistant" else "ğŸ§ You"
Â  Â  Â  Â  Â  Â  st.markdown(f"**{who}:** {msg['content']}")

Â  Â  # Get user input
Â  Â  user_input = st.text_input("Ask your question about this section:")

Â  Â  if st.button("Send Question", key="deep_dive_send"):
Â  Â  Â  Â  if user_input:
Â  Â  Â  Â  Â  Â  # Add user message to history and API message list
Â  Â  Â  Â  Â  Â  S["deep_dive_history"].append({"role": "user", "content": user_input})
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # Prepare messages for AI
Â  Â  Â  Â  Â  Â  context_prompt = {
Â  Â  Â  Â  Â  Â  Â  Â  "role": "system",
Â  Â  Â  Â  Â  Â  Â  Â  "content": f"You are a master theologian answering a user's question about a specific lesson section they just read. Do not re-introduce yourself. Answer with depth and clarity. The lesson text is: '{S.get('deep_dive_context', 'No context provided.')}'"
Â  Â  Â  Â  Â  Â  }
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  messages_for_api = [
Â  Â  Â  Â  Â  Â  Â  Â  {"role": "system", "content": THEOLOGICAL_SYSTEM_PROMPT}, # Your main theological prompt
Â  Â  Â  Â  Â  Â  Â  Â  context_prompt
Â  Â  Â  Â  Â  Â  ] + S["deep_dive_history"] # Add the mini-chat history

Â  Â  Â  Â  Â  Â  with st.spinner("Thinking..."):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = client.chat.completions.create(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=MODEL,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=messages_for_api,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.3
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ai_reply = response.choices[0].message.content.strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["deep_dive_history"].append({"role": "assistant", "content": ai_reply})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to show the new messages
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error getting answer: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["deep_dive_history"].pop() # Remove user message if AI failed
Â Â  Â 
Â  Â  st.markdown("---")
Â  Â  if st.button("Return to Lesson"):
Â  Â  Â  Â  # Clear the deep dive state and return to the lesson
Â  Â  Â  Â  S["deep_dive_mode"] = False
Â  Â  Â  Â  if "deep_dive_context" in S: del S["deep_dive_context"]
Â  Â  Â  Â  if "deep_dive_history" in S: del S["deep_dive_history"]
Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â 
# ================================================================
# LEARN MODULE: NEW DASHBOARD VIEW
# ================================================================
def run_dashboard_view(S):
Â  Â  """Displays the main syllabus/dashboard for the learning plan."""
Â  Â  st.title(S["plan"].get("plan_title", "Your Learning Journey"))
Â  Â  st.write(S["plan"].get("introduction", ""))

Â  Â  # Check for plan completion
Â  Â  if S.get("plan_completed", False): # Check for a top-level completion flag
Â  Â  Â  Â  st.success("ğŸ‰ You've completed your entire learning journey!")
Â  Â  Â  Â  st.balloons()
Â  Â  Â  Â  if st.button("Start a New Journey"):
Â  Â  Â  Â  Â  Â  st.session_state.learn_state = {} # Reset state completely
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  return
Â Â  Â 
Â  Â  # Check if all levels are completed (in case flag wasn't set)
Â  Â  all_levels_complete = all(l.get("quiz_completed", False) for l in S.get("levels", []))
Â  Â  if all_levels_complete and S.get("levels"):
Â  Â  Â  Â  S["plan_completed"] = True
Â  Â  Â  Â  st.rerun()

Â  Â  # Loop through all levels
Â  Â  for level_index, level_data in enumerate(S["levels"]):
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  level_name = level_data.get('name', f'Level {level_index + 1}')
Â Â  Â  Â  Â 
Â  Â  Â  Â  # Determine level status
Â  Â  Â  Â  is_current_level = (level_index == S["current_level"])
Â  Â  Â  Â  is_level_completed = level_data.get("quiz_completed", False)
Â Â  Â  Â  Â 
Â  Â  Â  Â  # A level is locked if a *previous* level is not yet completed
Â  Â  Â  Â  is_level_locked = False
Â  Â  Â  Â  if level_index > 0:
Â  Â  Â  Â  Â  Â  is_level_locked = not S["levels"][level_index - 1].get("quiz_completed", False)
Â Â  Â  Â  Â 
Â  Â  Â  Â  if is_level_completed:
Â  Â  Â  Â  Â  Â  st.markdown(f"## âœ… {level_name}")
Â  Â  Â  Â  elif is_current_level:
Â  Â  Â  Â  Â  Â  st.markdown(f"## ğŸ‘‰ {level_name}")
Â  Â  Â  Â  elif is_level_locked:
Â Â  Â  Â  Â  Â  Â  st.markdown(f"## ğŸ”’ {level_name}")
Â  Â  Â  Â  else: # Should mean it's a future, unlocked level (but not current)
Â  Â  Â  Â  Â  Â  st.markdown(f"## {level_name}")
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  st.markdown(f"**Topic:** {level_data.get('topic', 'N/A')}")

Â  Â  Â  Â  if is_level_locked:
Â  Â  Â  Â  Â  Â  st.info("This level is locked. Complete the previous level's quiz to unlock.")
Â  Â  Â  Â  Â  Â  continue # Skip to the next level in the loop

Â  Â  Â  Â  # --- Lesson Loop ---
Â  Â  Â  Â  num_lessons = level_data.get("num_lessons", 0)
Â  Â  Â  Â  if "lessons" not in level_data: level_data["lessons"] = []
Â Â  Â  Â  Â 
Â  Â  Â  Â  for lesson_index in range(num_lessons):
Â  Â  Â  Â  Â  Â  lesson_title = f"Lesson {lesson_index + 1}"
Â  Â  Â  Â  Â  Â  lesson_generated = (lesson_index < len(level_data["lessons"]))
Â  Â  Â  Â  Â  Â  lesson_data = None
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if lesson_generated:
Â  Â  Â  Â  Â  Â  Â  Â  lesson_data = level_data["lessons"][lesson_index]
Â  Â  Â  Â  Â  Â  Â  Â  lesson_title = lesson_data.get("lesson_title", lesson_title)

Â  Â  Â  Â  Â  Â  # Determine lesson status
Â  Â  Â  Â  Â  Â  is_lesson_completed = lesson_data.get("completed", False) if lesson_data else False
Â  Â  Â  Â  Â  Â  is_current_lesson_in_progress = (is_current_level and lesson_index == S["current_lesson_index"] and not S.get("quiz_mode"))
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # A lesson is locked if the *previous* lesson in this level is not complete
Â  Â  Â  Â  Â  Â  is_lesson_locked = False
Â  Â  Â  Â  Â  Â  if lesson_index > 0:
Â  Â  Â  Â  Â  Â  Â  Â  # Check if previous lesson exists and is marked complete
Â  Â  Â  Â  Â  Â  Â  Â  if not (lesson_index - 1 < len(level_data["lessons"]) and level_data["lessons"][lesson_index - 1].get("completed", False)):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_lesson_locked = True
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # The first lesson of a level is never locked (if the level itself isn't locked)
Â  Â  Â  Â  Â  Â  if lesson_index == 0:
Â  Â  Â  Â  Â  Â  Â  Â  is_lesson_locked = False

Â  Â  Â  Â  Â  Â  # Don't show "In Progress" for a completed lesson
Â  Â  Â  Â  Â  Â  if is_lesson_completed:
Â  Â  Â  Â  Â  Â  Â  Â  is_current_lesson_in_progress = False

Â  Â  Â  Â  Â  Â  status_text = ""
Â  Â  Â  Â  Â  Â  if is_lesson_completed:
Â  Â  Â  Â  Â  Â  Â  Â  status_text = "Status: âœ… Completed"
Â  Â  Â  Â  Â  Â  elif is_current_lesson_in_progress:
Â  Â  Â  Â  Â  Â  Â  Â  status_text = f"Status: In Progress (Section {S.get('current_section_index', 0) + 1})"
Â  Â  Â  Â  Â  Â  elif is_lesson_locked:
Â  Â  Â  Â  Â  Â  Â  Â  status_text = "Status: ğŸ”’ Locked (Complete previous lesson)"
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  status_text = "Status: Not Started"

Â  Â  Â  Â  Â  Â  with st.container(border=True):
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{lesson_title}**")
Â  Â  Â  Â  Â  Â  Â  Â  st.caption(status_text)
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  if is_lesson_completed:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Review Lesson", key=f"review_{level_index}_{lesson_index}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "lesson"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_level"] = level_index
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_lesson_index"] = lesson_index
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] = 0 # Start from beginning
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  elif is_current_lesson_in_progress:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Resume Lesson", type="primary", key=f"resume_{level_index}_{lesson_index}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "lesson"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  elif not is_lesson_locked: # Not completed, not in progress, but not locked = ready to start
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Start Lesson", type="primary", key=f"start_{level_index}_{lesson_index}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "lesson"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_level"] = level_index
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_lesson_index"] = lesson_index
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # This will trigger the lesson generation in run_lesson_view

Â  Â  Â  Â  # --- Quiz Button ---
Â  Â  Â  Â  # Show quiz if all lessons are complete and quiz is not
Â  Â  Â  Â  all_lessons_complete = all(l.get("completed", False) for l in level_data.get("lessons", [])) and len(level_data.get("lessons", [])) == num_lessons
Â Â  Â  Â  Â 
Â  Â  Â  Â  if all_lessons_complete and not is_level_completed:
Â  Â  Â  Â  Â  Â  st.info("You've completed all lessons for this level. Time for the final quiz!")
Â  Â  Â  Â  Â  Â  if st.button("Start Level Quiz", type="primary", key=f"start_quiz_{level_index}"):
Â  Â  Â  Â  Â  Â  Â  Â  S["quiz_mode"] = True
Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "lesson" # Switch to lesson view (which handles quiz mode)
Â  Â  Â  Â  Â  Â  Â  Â  S["current_level"] = level_index
Â  Â  Â  Â  Â  Â  Â  Â  S["current_question_index"] = 0 # Reset quiz
Â  Â  Â  Â  Â  Â  Â  Â  S["user_score"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  elif is_level_completed:
Â  Â  Â  Â  Â  Â  st.success("You have successfully completed this level.")
Â  Â  Â  Â  Â  Â  if st.button("Retake Quiz?", key=f"retake_quiz_{level_index}"):
Â  Â  Â  Â  Â  Â  Â  Â  S["quiz_mode"] = True
Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "lesson"
Â  Â  Â  Â  Â  Â  Â  Â  S["current_level"] = level_index # Ensure we're on the right level
Â  Â  Â  Â  Â  Â  Â  Â  S["current_question_index"] = 0 # Reset quiz
Â  Â  Â  Â  Â  Â  Â  Â  S["user_score"] = 0
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â  Â  Â  Â  Â 
# ================================================================
# LEARN MODULE: NEW LESSON VIEW
# ================================================================
def run_lesson_view(S):
Â  Â  """Displays the active lesson, quiz, or deep dive chat."""
Â Â  Â 
Â  Â  # --- Check for Deep Dive Mode ---
Â  Â  if S.get("deep_dive_mode", False):
Â  Â  Â  Â  run_deep_dive_chat(S)
Â  Â  Â  Â  return
Â  Â  # --- End of Deep Dive Check ---
Â Â  Â 
Â  Â  # --- Back to Syllabus Button ---
Â  Â  if st.button("â¬…ï¸ Back to Syllabus"):
Â  Â  Â  Â  S["view_mode"] = "dashboard"
Â  Â  Â  Â  S["quiz_mode"] = False # Always exit quiz mode when going to dashboard
Â  Â  Â  Â  st.rerun()
Â  Â  # --- End of Back Button ---

Â  Â  # Make sure we have a valid level
Â  Â  if S["current_level"] >= len(S["levels"]):
Â  Â  Â  Â  st.error("Error: Level not found. Returning to dashboard.")
Â  Â  Â  Â  S["view_mode"] = "dashboard"
Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  return
Â Â  Â  Â  Â 
Â  Â  level_data = S["levels"][S["current_level"]]
Â  Â  num_lessons = level_data.get("num_lessons", 1)
Â Â  Â 
Â  Â  # --- Run Quiz Mode ---
Â  Â  if S.get("quiz_mode"):
Â Â  Â  Â  Â 
Â  Â  Â  Â  # --- Generate Quiz on Demand ---
Â  Â  Â  Â  if "quiz_questions" not in level_data:
Â  Â  Â  Â  Â  Â  with st.spinner("Generating your level quiz..."):
Â  Â  Â  Â  Â  Â  Â  Â  all_summaries = [l.get("lesson_summary", "") for l in level_data.get("lessons", [])]
Â  Â  Â  Â  Â  Â  Â  Â  quiz_prompt = create_level_quiz_prompt(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  level_topic=level_data.get("topic"),Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lesson_summaries=all_summaries,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  level_name=level_data.get("name")
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  quiz_resp = ask_gpt_json(quiz_prompt, max_tokens=2500)Â 
Â  Â  Â  Â  Â  Â  Â  Â  quiz_data_object = _learn_extract_json_any(quiz_resp)
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  if (quiz_data_object andÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  isinstance(quiz_data_object, dict) andÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "quiz" in quiz_data_object andÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  isinstance(quiz_data_object["quiz"], list)):
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  level_data["quiz_questions"] = quiz_data_object["quiz"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to show the quiz now that it's generated
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("Failed to generate valid quiz questions. Please try starting the quiz again.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if quiz_resp: st.text_area("Raw AI Quiz Response (for debugging):", quiz_resp, height=200)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["quiz_mode"] = False # Exit quiz mode on failure
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  # --- End of Quiz Generation ---
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  run_level_quiz(S)
Â  Â  Â  Â  return

Â  Â  # --- Lesson Generation and Display ---
Â  Â  if "lessons" not in level_data: level_data["lessons"] = []

Â  Â  # Make sure we have a valid lesson index
Â  Â  if S["current_lesson_index"] >= num_lessons:
Â  Â  Â  Â  st.warning("You've completed all lessons for this level. Returning to dashboard.")
Â  Â  Â  Â  S["view_mode"] = "dashboard"
Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  return

Â  Â  # Generate lesson if it doesn't exist yet
Â  Â  if S["current_lesson_index"] >= len(level_data["lessons"]):
Â  Â  Â  Â  with st.spinner(f"Generating Lesson {S['current_lesson_index'] + 1}..."):
Â  Â  Â  Â  Â  Â  prev_summary = None
Â  Â  Â  Â  Â  Â  if S["current_lesson_index"] > 0:
Â  Â  Â  Â  Â  Â  Â  Â  prev_summary = level_data["lessons"][S["current_lesson_index"] - 1].get("lesson_summary")
Â  Â  Â  Â  Â  Â  elif S["current_level"] > 0:
Â  Â  Â  Â  Â  Â  Â  Â  prev_level_lessons = S["levels"][S["current_level"]-1].get("lessons", [])
Â  Â  Â  Â  Â  Â  Â  Â  if prev_level_lessons:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prev_summary = prev_level_lessons[-1].get("lesson_summary")

Â  Â  Â  Â  Â  Â  # <<< NEW >>> Get struggle topics to pass to prompt
Â  Â  Â  Â  Â  Â  struggles = [topic for topic, count in S.get("struggle_log", {}).items() if count > 0]
Â  Â  Â  Â  Â  Â  struggle_summary = ", ".join(struggles) if struggles else None

Â  Â  Â  Â  Â  Â  lesson_max_tokens = TOKENS_BY_TIME.get(S["form_data"]['time_commitment'], 4000)
Â  Â  Â  Â  Â  Â  lesson_prompt = create_lesson_prompt(
Â  Â  Â  Â  Â  Â  Â  Â  level_topic=level_data.get("topic"),
Â  Â  Â  Â  Â  Â  Â  Â  lesson_number=S["current_lesson_index"] + 1,
Â  Â  Â  Â  Â  Â  Â  Â  total_lessons_in_level=num_lessons,
Â  Â  Â  Â  Â  Â  Â  Â  form_data=S["form_data"],
Â  Â  Â  Â  Â  Â  Â  Â  previous_lesson_summary=prev_summary,
Â  Â  Â  Â  Â  Â  Â  Â  previous_struggles=struggle_summary # <<< NEW >>> Pass struggles
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  lesson_resp = ask_gpt_json(lesson_prompt, max_tokens=lesson_max_tokens)
Â  Â  Â  Â  Â  Â  lesson_data = _learn_extract_json_any(lesson_resp) if lesson_resp else None
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  if (lesson_data and isinstance(lesson_data, dict) andÂ 
Â  Â  Â  Â  Â  Â  Â  Â  "lesson_content_sections" in lesson_data andÂ 
Â  Â  Â  Â  Â  Â  Â  Â  isinstance(lesson_data["lesson_content_sections"], list) and
Â  Â  Â  Â  Â  Â  Â  Â  len(lesson_data["lesson_content_sections"]) > 0):
Â Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  lesson_data["lesson_summary"] = summarize_lesson_content(lesson_data)
Â  Â  Â  Â  Â  Â  Â  Â  level_data["lessons"].append(lesson_data)
Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] = 0Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.error("Failed to generate valid lesson content. The AI response might be malformed or empty. Returning to syllabus.")
Â  Â  Â  Â  Â  Â  Â  Â  if lesson_resp: st.text_area("Raw AI Lesson Response (for debugging):", lesson_resp, height=200)
Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard"Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  returnÂ 
Â Â  Â 
Â  Â  # --- Display Current Lesson Section ---
Â  Â  current_lesson = level_data["lessons"][S["current_lesson_index"]]
Â  Â  st.markdown(f"### Lesson {S['current_lesson_index'] + 1}: {current_lesson.get('lesson_title', 'Untitled Lesson')}")
Â Â  Â 
Â  Â  lesson_sections = current_lesson.get("lesson_content_sections", [])
Â  Â  if not lesson_sections:
Â Â  Â  Â  Â  st.warning("This lesson appears to be empty. Returning to syllabus.")
Â Â  Â  Â  Â  S["view_mode"] = "dashboard"
Â Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â  return

Â  Â  if S["current_section_index"] < len(lesson_sections):
Â  Â  Â  Â  section = lesson_sections[S["current_section_index"]]
Â  Â  Â  Â  section_type = section.get("type")

Â  Â  Â  Â  if section_type == "text":
Â  Â  Â  Â  Â  Â  # <<< NEW >>> Handle new "Review" role
Â  Â  Â  Â  Â  Â  if section.get("role") == "Review":
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"**Review Section:** {section.get('content', '*No content for this section.*')}")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(section.get("content", "*No content for this section.*"))
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  # --- START: New Audio Integration for Lesson Audio ---
Â  Â  Â  Â  Â  Â  audio_ref = section.get("audio_reference")
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  if audio_ref:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ğŸ§ Listen to the Scripture:** *{audio_ref}*")
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  # --- MOCK AUDIO PLAYER ---
Â  Â  Â  Â  Â  Â  Â  Â  # Replace this URL with a call to a function that fetches a valid MP3/M4A 
Â  Â  Â  Â  Â  Â  Â  Â  # link for the specific verse (e.g., fetch_audio_url(audio_ref)).
Â  Â  Â  Â  Â  Â  Â  Â  mock_audio_url = "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3" 
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  st.audio(mock_audio_url, format="audio/mp3")
Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"**Mock:** A dynamic audio link for **{audio_ref}** would be displayed here via `st.audio()` once a Bible Audio API is integrated.")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  # --- END: Audio Integration ---
Â Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  nav_cols = st.columns([1, 1, 1])
Â  Â  Â  Â  Â  Â  with nav_cols[0]:
Â  Â  Â  Â  Â  Â  Â  Â  if S["current_section_index"] > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("â¬…ï¸ Previous Section", key=f"prev_sec_{S['current_section_index']}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # <<< NEW >>> Clear remediation flags when moving
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["awaiting_remediation"] = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "remediation_question" in S: del S["remediation_question"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "breakdown_content" in S: del S["breakdown_content"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] -= 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  with nav_cols[1]:
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Continue Reading â¡ï¸", key=f"cont_{S['current_level']}_{S['current_lesson_index']}_{S['current_section_index']}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] += 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  with nav_cols[2]:
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("ğŸ¤” Ask a question...", key=f"deep_dive_{S['current_section_index']}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["deep_dive_mode"] = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["deep_dive_context"] = section.get("content", "No content provided.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  elif section_type == "knowledge_check":
Â  Â  Â  Â  Â  Â  display_knowledge_check_question(S)Â 
Â Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  else:
Â Â  Â  Â  Â  Â  Â  st.warning(f"Unknown section type '{section_type}'. Skipping.")
Â Â  Â  Â  Â  Â  Â  S["current_section_index"] += 1
Â Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â 
Â  Â  # --- End of Lesson Reached ---
Â  Â  else:Â 
Â  Â  Â  Â  st.success(f"Lesson {S['current_lesson_index'] + 1} Completed!")
Â  Â  Â  Â  current_lesson["completed"] = True # Set lesson complete flag
Â Â  Â  Â  Â 
Â  Â  Â  Â  st.markdown("**Key Takeaways from this Lesson:**")
Â  Â  Â  Â  summary_points = current_lesson.get("summary_points", [])
Â  Â  Â  Â  if summary_points:
Â  Â  Â  Â  Â  Â  for point in summary_points: st.markdown(f"- {point}")
Â  Â  Â  Â  else:
Â Â  Â  Â  Â  Â  Â  st.write("*No summary points provided.*")
Â Â  Â  Â  Â 
Â  Â  Â  Â  nav_cols = st.columns(2)
Â  Â  Â  Â  with nav_cols[0]:
Â  Â  Â  Â  Â  Â  if st.button("â¬…ï¸ Review This Lesson", key="prev_lesson"):
Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] = 0 # Reset to start of this lesson
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â Â  Â  Â  Â 
Â  Â  Â  Â  with nav_cols[1]:
Â  Â  Â  Â  Â  Â  # Check if this was the last lesson
Â  Â  Â  Â  Â  Â  if S["current_lesson_index"] + 1 < num_lessons:
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Continue to Next Lesson â–¶ï¸", type="primary"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard" # Go to dashboard
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_lesson_index"] += 1 # Advance to next lesson
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["current_section_index"] = 0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("You've completed all lessons for this level. Time for the final quiz!")
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("Go to Level Quiz", type="primary"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  S["view_mode"] = "dashboard" # Go to dashboard
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

# ================================================================
# MAIN LEARN MODULE FLOW (THE NEW ROUTER)
# ================================================================
def run_learn_module():
Â  Â  st.subheader("ğŸ“š Learn Module â€” Personalized Bible Learning")
Â  Â  if "learn_state" not in st.session_state: st.session_state.learn_state = {}
Â  Â  S = st.session_state.learn_state

Â  Â  # <<< NEW >>> Initialize struggle log
Â  Â  if "struggle_log" not in S:
Â  Â  Â  Â  S["struggle_log"] = {}

Â  Â  # --- 1. Run Diagnostic Quiz if not completed ---
Â  Â  if not S.get("diagnostic_complete", False):
Â  Â  Â  Â  run_diagnostic_quiz()Â 
Â  Â  Â  Â  returnÂ 
Â Â  Â 
Â  Â  # --- 2. Run Plan Setup if plan not generated ---
Â  Â  if "plan" not in S:
Â  Â  Â  Â  run_learn_module_setup()
Â  Â  Â  Â  return

Â  Â  # --- 3. Main Router: Show Dashboard or Lesson View ---
Â  Â  if S.get("view_mode") == "lesson":
Â  Â  Â  Â  run_lesson_view(S)
Â  Â  else:
Â  Â  Â  Â  # Default view is the dashboard
Â  Â  Â  Â  S["view_mode"] = "dashboard" # Ensure it's set
Â  Â  Â  Â  run_dashboard_view(S)
Â Â  Â  Â  Â 
# ================================================================
# MAIN UI
# ================================================================
st.set_page_config(page_title="Bible GPT", layout="wide")
st.title("âœ… Bible GPT")

# Sidebar Navigation
mode = st.sidebar.selectbox("Choose a mode:", [
Â  Â  "Learn Module", "Bible Lookup", "Chat with GPT", "Sermon Transcriber & Summarizer",
Â  Â  "Practice Chat", "Verse of the Day", "Study Plan", "Faith Journal", "Prayer Starter",
Â  Â  "Fast Devotional", "Small Group Generator",Â 
Â  Â  # "Tailored Learning Path", # Consider removing legacy option?
Â  Â  "Bible Beta Mode",
Â  Â  "Pixar Story Animation",
])

# Mode Routing Dictionary
mode_functions = {
Â  Â  "Learn Module": run_learn_module,
Â  Â  "Bible Lookup": run_bible_lookup,
Â  Â  "Chat with GPT": run_chat_mode,
Â  Â  "Sermon Transcriber & Summarizer": run_sermon_transcriber,
Â  Â  "Practice Chat": run_practice_chat,
Â  Â  "Verse of the Day": run_verse_of_the_day,
Â  Â  "Study Plan": run_study_plan,
Â  Â  "Faith Journal": run_faith_journal,
Â  Â  "Prayer Starter": run_prayer_starter,
Â  Â  "Fast Devotional": run_fast_devotional,
Â  Â  "Small Group Generator": run_small_group_generator,
Â  Â  # "Tailored Learning Path": run_learning_path_mode, # Keep if needed
Â  Â  "Bible Beta Mode": run_bible_beta,
Â  Â  "Pixar Story Animation": run_pixar_story_animation,
}

# Execute Selected Mode
if mode in mode_functions:
Â  Â  try:
Â  Â  Â  Â  mode_functions[mode]()
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"An unexpected error occurred in {mode}: {e}")
Â  Â  Â  Â  # Optionally add more detailed error logging here
Â  Â  Â  Â  # import traceback
Â  Â  Â  Â  # st.code(traceback.format_exc())Â 
else:
Â  Â  st.warning("Selected mode not found or mapped.")
